{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Background Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "### header ###\n",
    "__author__ = \"Jenhan Tao\"\n",
    "__license__ = \"BSD\"\n",
    "__email__ = \"jenhantao@gmail.com\"\n",
    "\n",
    "### imports ###\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib\n",
    "import itertools\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "import matplotlib_venn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from random import shuffle\n",
    "\n",
    "### notebook specific configuration ###\n",
    "%matplotlib inline\n",
    "matplotlib.pylab.rcParams['savefig.dpi'] = 200\n",
    "sys.setrecursionlimit(5000)\n",
    "os.chdir('/gpfs/data01/glasslab/home/jtao/analysis/random_background_analysis/')\n",
    "sns.set_context('notebook')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into GC content matched training and test data\n",
    "def get_GC_matched_split(features, labels, test_size, tolerance = 0.01):\n",
    "    '''\n",
    "    feature: 2D array (samples x features)\n",
    "    labels: 1D boolean array (samples x)\n",
    "    test_size: fraction of data to test on\n",
    "    tolerance: max difference in GC content between True and False labelled samples\n",
    "    '''\n",
    "    global _id_sequence_dict\n",
    "    \n",
    "    ### match GC content of samples labelled True with those labelled False by thowing out False samples\n",
    "    # retrieve sequences using index of labels\n",
    "    index_label_tuples = tuple(zip(labels.index.values, labels.values))\n",
    "    \n",
    "    true_sequences = [_id_sequence_dict[x[0]] for x in index_label_tuples if x[1]]\n",
    "    true_ids = [x[0] for x in index_label_tuples if x[1]]\n",
    "    \n",
    "    false_sequences = [_id_sequence_dict[x[0]] for x in index_label_tuples if not x[1]]\n",
    "    false_ids = [x[0] for x in index_label_tuples if not x[1]]\n",
    "    \n",
    "    # calculate GC content of True samples\n",
    "    true_gc_count = 0\n",
    "    true_length = 0\n",
    "    for s in true_sequences:\n",
    "        true_gc_count += s.count('G')\n",
    "        true_gc_count += s.count('C')\n",
    "        true_length += len(s)\n",
    "    true_gc_content = true_gc_count/(true_length+0.0000001)\n",
    "    \n",
    "    # calcuate GC content of False samples\n",
    "    false_gc_count = 0\n",
    "    false_length = 0\n",
    "    for s in false_sequences:\n",
    "        false_gc_count += s.count('G')\n",
    "        false_gc_count += s.count('C')\n",
    "        false_length += len(s)\n",
    "    false_gc_content = false_gc_count/(false_length+0.0000001)\n",
    "    \n",
    "    while abs(true_gc_content - false_gc_content) > tolerance:\n",
    "        # remove false GC sequences until GC content matches tolerance\n",
    "        selected_seq = False\n",
    "        \n",
    "        while not selected_seq:\n",
    "            rand_index = np.random.randint(len(false_sequences))\n",
    "            current_seq = false_sequences[rand_index]\n",
    "            current_gc_count = current_seq.count('G')+ current_seq.count('C')\n",
    "            current_length = len(current_seq)\n",
    "            current_gc = current_gc_count/current_length\n",
    "            if true_gc_content > false_gc_content:\n",
    "                # remove sequences that would increase overall GC content of False sequences\n",
    "                if current_gc < false_gc_content:\n",
    "                    selected_seq = True\n",
    "            else:\n",
    "                # remove sequences that would decrease overall GC content of False sequences\n",
    "                if current_gc > false_gc_content:\n",
    "                    selected_seq = True\n",
    "        false_gc_count -= current_gc_count\n",
    "        false_length -= current_length\n",
    "        false_gc_content = false_gc_count/false_length\n",
    "        \n",
    "        false_sequences.pop(rand_index)\n",
    "        false_ids.pop(rand_index)\n",
    "    \n",
    "    filtered_ids = true_ids + false_ids\n",
    "    filtered_features = features[features.index.isin(filtered_ids)]\n",
    "    filtered_labels = labels[labels.index.isin(filtered_ids)]\n",
    "\n",
    "    if test_size <= 0.5:\n",
    "        training_indices, test_indices = next(iter(\n",
    "                sklearn.cross_validation.StratifiedKFold(filtered_labels, int(1/test_size), shuffle=True)))\n",
    "    else:\n",
    "        test_indices, training_indices = next(\n",
    "            iter(sklearn.cross_validation.StratifiedKFold(filtered_labels, int(1/(1-test_size)), shuffle=True)))\n",
    "    training_ids = [filtered_ids[i] for i in training_indices]\n",
    "    test_ids = [filtered_ids[i] for i in test_indices]\n",
    "    \n",
    "    training_features = filtered_features[filtered_features.index.isin(training_ids)]\n",
    "    test_features = filtered_features[filtered_features.index.isin(test_ids)]\n",
    "    training_labels = filtered_labels[filtered_labels.index.isin(training_ids)]\n",
    "    test_labels = filtered_labels[filtered_labels.index.isin(test_ids)]\n",
    "    \n",
    "    return training_features, test_features, training_labels, test_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Score Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_score_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_sequence_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_strand_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_start_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_end_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/summary_frame.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/annotation_frame.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/peak_sequences/C57BL6J.fa ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Score Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "motif_score_frame=pd.read_pickle('motif_score_frame_C57BL6J.pickle')\n",
    "motif_sequence_frame = pd.read_pickle('motif_sequence_frame_C57BL6J.pickle')\n",
    "motif_strand_frame = pd.read_pickle('motif_strand_frame_C57BL6J.pickle')\n",
    "motif_start_frame = pd.read_pickle('motif_start_frame_C57BL6J.pickle')\n",
    "motif_end_frame = pd.read_pickle('motif_end_frame_C57BL6J.pickle')\n",
    "summary_frame = pd.read_pickle('summary_frame.pickle')\n",
    "annotation_frame = pd.read_pickle('annotation_frame.pickle')\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "normed_motif_frame = pd.DataFrame(scaler.fit_transform(motif_score_frame.ix[:,3:]))\n",
    "normed_motif_frame.columns = motif_score_frame.columns.values[3:]\n",
    "normed_motif_frame.index = motif_score_frame.index.values\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "standardized_motif_frame = pd.DataFrame(scaler.fit_transform(motif_score_frame.ix[:,3:]))\n",
    "standardized_motif_frame.columns = motif_score_frame.columns.values[3:]\n",
    "standardized_motif_frame.index = motif_score_frame.index.values\n",
    "\n",
    "_factors = sorted(list(set([x.split('_')[1] for x in summary_frame.columns if '_' in x])))\n",
    "_factors.remove('atac')\n",
    "\n",
    "### read in sequences as dictionary {peakID: sequence}\n",
    "with open('./C57BL6J.fa') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "_id_sequence_dict = {}\n",
    "for line in data:\n",
    "    if line[0] == '>':\n",
    "        sequenceName = line.strip()[1:]\n",
    "    else:\n",
    "        _id_sequence_dict[sequenceName] = line.strip().upper()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classifier using Open Chromatin Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numIterations = 5\n",
    "ap1_members = ['atf3','cjun', 'fos', 'junb','jund']    \n",
    "test_size = 0.5\n",
    "factors = ['atf3','cjun', 'fos', 'junb','jund', 'atac', 'cebpa', 'pu1', 'p65']\n",
    "# c57bl6_indices = summary_frame[summary_frame['Factors'].str.contains('c57bl6')].index.values  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:49: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atf3_veh roc: 0.836974709714 5.61270208208e-06 precision: 0.711601594732 3.53410618648e-06 numTestPositives: 10991\n",
      "cjun_veh roc: 0.812956501449 3.90929117893e-06 precision: 0.470326834035 6.15164296518e-06 numTestPositives: 6346\n",
      "fos_veh roc: 0.85820935437 9.32261322104e-06 precision: 0.354988988947 4.44600984708e-05 numTestPositives: 971\n",
      "junb_veh roc: 0.685413905353 8.2720549774e-05 precision: 0.0214656931746 2.62037146045e-07 numTestPositives: 244\n",
      "jund_veh roc: 0.806430962408 3.85320637865e-06 precision: 0.56506012027 2.15782634439e-05 numTestPositives: 9146\n",
      "atf3_kla roc: 0.831525390212 4.86761024132e-07 precision: 0.802433251593 3.69917466276e-06 numTestPositives: 17245\n",
      "cjun_kla roc: 0.807262952194 1.08763770612e-06 precision: 0.480549590428 4.3931060878e-06 numTestPositives: 8022\n",
      "fos_kla roc: 0.832284162804 1.86405547115e-06 precision: 0.661452757926 1.67635553822e-05 numTestPositives: 10670\n",
      "junb_kla roc: 0.829456310449 2.86007019466e-06 precision: 0.480705564122 1.51660288731e-05 numTestPositives: 7071\n",
      "jund_kla roc: 0.816022942104 2.52666953236e-06 precision: 0.708676688008 9.36877713894e-06 numTestPositives: 14925\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for monomers using all motifs\n",
    "strain = 'c57bl6'\n",
    "factor_auc_dict = {}\n",
    "factor_precision_dict = {}\n",
    "factor_coeff_dict = {}\n",
    "factor_prob_dict = {}\n",
    "factor_meanCoeff_dict = {}\n",
    "factor_intercept_dict = {}\n",
    "factor_meanIntercept_dict = {}\n",
    "for treatment in ['veh', 'kla']:\n",
    "    for monomer in ap1_members:\n",
    "        c57bl6_indices = summary_frame[summary_frame[['c57bl6_' + x + '_' + treatment for x in factors]].sum(axis=1) > 0].index.values  \n",
    "        features = standardized_motif_frame[standardized_motif_frame.index.isin(c57bl6_indices)]\n",
    "        labels = summary_frame[summary_frame.index.isin(c57bl6_indices)][strain + '_' + monomer + '_' + treatment] > 0.0\n",
    "        if np.sum(labels) >= 100:\n",
    "            all_aucs = []\n",
    "            all_coefficients = []\n",
    "            all_probs = None\n",
    "            all_precisions = []\n",
    "            all_intercepts = []\n",
    "            for i in range(numIterations):  \n",
    "\n",
    "                # split data into training and test sets\n",
    "                training_features, test_features, training_labels, test_labels = get_GC_matched_split(\n",
    "                    features, labels, test_size = test_size, tolerance = 0.01)\n",
    "\n",
    "                #  Run classifier\n",
    "                lr_classifier = sklearn.linear_model.LogisticRegression(penalty='l1', n_jobs=-1)\n",
    "\n",
    "                lr_classifier.fit(training_features, training_labels)\n",
    "                # retrieve probabilities\n",
    "                probas_lr = lr_classifier.predict_proba(test_features)\n",
    "\n",
    "                # score predictions\n",
    "                current_roc_auc = sklearn.metrics.roc_auc_score(test_labels, probas_lr[:, 1], average = None)\n",
    "                current_precision = sklearn.metrics.average_precision_score(test_labels, probas_lr[:, 1], average = None)\n",
    "\n",
    "                all_aucs.append(current_roc_auc)\n",
    "                all_precisions.append(current_precision)\n",
    "\n",
    "                # score all sequences\n",
    "                probs = lr_classifier.predict_proba(features)[:, 1]\n",
    "\n",
    "                current_coefficients = lr_classifier.coef_.flatten()\n",
    "                all_coefficients.append(current_coefficients)\n",
    "                all_intercepts.append(lr_classifier.intercept_[0])\n",
    "                \n",
    "                if all_probs == None:\n",
    "                    all_probs = probs\n",
    "                else:\n",
    "                    all_probs = all_probs + probs\n",
    "            mean_coefficients = np.mean(all_coefficients, axis=0)\n",
    "            \n",
    "            factor_auc_dict[monomer + '_' + treatment]= all_aucs\n",
    "            factor_precision_dict[monomer + '_' + treatment] = all_precisions\n",
    "            factor_coeff_dict[monomer + '_' + treatment] = all_coefficients\n",
    "            factor_prob_dict[monomer + '_' + treatment] = all_probs\n",
    "            factor_meanCoeff_dict[monomer + '_' + treatment] = mean_coefficients\n",
    "            factor_intercept_dict[monomer + '_' + treatment] = all_intercepts\n",
    "            factor_meanIntercept_dict[monomer + '_' + treatment] = np.mean(all_intercepts)\n",
    "            print(monomer + '_' + treatment,\n",
    "                  'roc:', np.mean(all_aucs), np.var(all_aucs),\n",
    "                  'precision:', np.mean(all_precisions), np.var(all_precisions),  \n",
    "                  'numTestPositives:', np.sum(test_labels)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create background peaks from genomic sequences from each chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomBackground(target_sequences, \n",
    "                        size_ratio = 1.0, \n",
    "                        tolerance = 0.01, \n",
    "                        N_threshold = 0.5 ):\n",
    "    '''\n",
    "    target_sequences: list of target sequences\n",
    "    size_ratio: number of background sequences relative to target sequences\n",
    "    tolerance: max difference in GC content between True and False labelled samples\n",
    "    '''\n",
    "    # throw away background peaks containing greater than this fraction of N\n",
    "\n",
    "\n",
    "    ### match GC content of samples labelled True with those labelled False by throwing out False samples\n",
    "    # retrieve sequences using index of labels\n",
    "        \n",
    "    # calculate GC content and length of the target sequences\n",
    "    target_gc_count = 0\n",
    "    target_length_count = 0\n",
    "    for s in target_sequences:\n",
    "        target_gc_count += s.count('G')\n",
    "        target_gc_count += s.count('C')\n",
    "        target_length_count += len(s)\n",
    "    target_gc_content = target_gc_count/(target_length_count+0.0000001) # GC content of target sequences\n",
    "    mean_target_length = target_length_count/len(target_sequences) # average length of target sequences\n",
    "    mean_target_length = int(mean_target_length)\n",
    "    print(target_gc_content, mean_target_length)\n",
    "    \n",
    "    # select random genomic loci \n",
    "    \n",
    "        \n",
    "#     # calcuate GC content of False samples\n",
    "#     false_gc_count = 0\n",
    "#     false_length = 0\n",
    "#     for s in false_sequences:\n",
    "#         false_gc_count += s.count('G')\n",
    "#         false_gc_count += s.count('C')\n",
    "#         false_length += len(s)\n",
    "#     false_gc_content = false_gc_count/(false_length+0.0000001)\n",
    "    \n",
    "#     while abs(target_gc_content - false_gc_content) > tolerance:\n",
    "#         # remove false GC sequences until GC content matches tolerance\n",
    "#         selected_seq = False\n",
    "        \n",
    "#         while not selected_seq:\n",
    "#             rand_index = np.random.randint(len(false_sequences))\n",
    "#             current_seq = false_sequences[rand_index]\n",
    "#             current_gc_count = current_seq.count('G')+ current_seq.count('C')\n",
    "#             current_length = len(current_seq)\n",
    "#             current_gc = current_gc_count/current_length\n",
    "#             if target_gc_content > false_gc_content:\n",
    "#                 # remove sequences that would increase overall GC content of False sequences\n",
    "#                 if current_gc < false_gc_content:\n",
    "#                     selected_seq = True\n",
    "#             else:\n",
    "#                 # remove sequences that would decrease overall GC content of False sequences\n",
    "#                 if current_gc > false_gc_content:\n",
    "#                     selected_seq = True\n",
    "#         false_gc_count -= current_gc_count\n",
    "#         false_length -= current_length\n",
    "#         false_gc_content = false_gc_count/false_length\n",
    "        \n",
    "#         false_sequences.pop(rand_index)\n",
    "#         false_ids.pop(rand_index)\n",
    "    \n",
    "#     filtered_ids = target_ids + false_ids\n",
    "#     filtered_features = features[features.index.isin(filtered_ids)]\n",
    "#     filtered_labels = labels[labels.index.isin(filtered_ids)]\n",
    "\n",
    "#     if test_size <= 0.5:\n",
    "#         training_indices, test_indices = next(iter(\n",
    "#                 sklearn.cross_validation.StratifiedKFold(filtered_labels, int(1/test_size), shuffle=True)))\n",
    "#     else:\n",
    "#         test_indices, training_indices = next(\n",
    "#             iter(sklearn.cross_validation.StratifiedKFold(filtered_labels, int(1/(1-test_size)), shuffle=True)))\n",
    "#     training_ids = [filtered_ids[i] for i in training_indices]\n",
    "#     test_ids = [filtered_ids[i] for i in test_indices]\n",
    "    \n",
    "#     training_features = filtered_features[filtered_features.index.isin(training_ids)]\n",
    "#     test_features = filtered_features[filtered_features.index.isin(test_ids)]\n",
    "#     training_labels = filtered_labels[filtered_labels.index.isin(training_ids)]\n",
    "#     test_labels = filtered_labels[filtered_labels.index.isin(test_ids)]\n",
    "    \n",
    "#     return training_features, test_features, training_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4856183106287546 375\n"
     ]
    }
   ],
   "source": [
    "getRandomBackground(target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22025 22025\n",
      "12670 12670\n",
      "1988 1988\n",
      "493 493\n",
      "18049 18049\n",
      "34654 34654\n",
      "16135 16135\n",
      "21465 21465\n",
      "14080 14080\n",
      "29964 29964\n"
     ]
    }
   ],
   "source": [
    "# for monomers using all motifs using random genomic background\n",
    "strain = 'c57bl6'\n",
    "factor_auc_dict = {}\n",
    "factor_precision_dict = {}\n",
    "factor_coeff_dict = {}\n",
    "factor_prob_dict = {}\n",
    "factor_meanCoeff_dict = {}\n",
    "factor_intercept_dict = {}\n",
    "factor_meanIntercept_dict = {}\n",
    "for treatment in ['veh', 'kla']:\n",
    "    for monomer in ap1_members:\n",
    "        c57bl6_indices = summary_frame[summary_frame[['c57bl6_' + x + '_' + treatment for x in factors]].sum(axis=1) > 0].index.values  \n",
    "        features = standardized_motif_frame[standardized_motif_frame.index.isin(c57bl6_indices)]\n",
    "\n",
    "        target_indices = summary_frame[summary_frame[strain + '_' + monomer + '_' + treatment] > 0.0].index.values\n",
    "        target_sequences = [_id_sequence_dict[x] for x in target_indices]\n",
    "        print(len(target_indices), len(target_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "peakSize = 200 # size of artificial background peaks\n",
    "N_threshold = 0.5 # throw away background peaks containing greater than this fraction of N\n",
    "\n",
    "tile_peak_file = open('./group/merged_tile_peaks.tsv','w')\n",
    "\n",
    "tile_peak_file.write('\\t'.join(['#ID', 'chr', 'start', 'end', 'strand\\n']))\n",
    "\n",
    "for f in os.listdir('./mm10_genome/'):\n",
    "    if '.fa' in f and not f in ['chrY.fa', 'chrM.fa'] :\n",
    "        current_chromosome = f.split('.')[0]\n",
    "        \n",
    "        # create seperate peak file containing just this chromosome\n",
    "        if not os.path.isdir('./group_by_chromosome/'+current_chromosome):\n",
    "            os.mkdir('./group_by_chromosome/'+current_chromosome)\n",
    "        current_tile_peak_file = open(\n",
    "            './group_by_chromosome/'+ current_chromosome + '/' + current_chromosome +'_tile_peaks.tsv','w')\n",
    "        current_tile_peak_file.write('\\t'.join(['#ID', 'chr', 'start', 'end', 'strand\\n']))\n",
    "        \n",
    "        # read in fastq file line by line\n",
    "        with open('./mm10_genome/' + f) as chromosome_file:\n",
    "            data = chromosome_file.readlines()\n",
    "        genome_seq = ''\n",
    "        for line in data[1:]:\n",
    "            genome_seq += line.strip().upper()\n",
    "        genome_seq_size = len(genome_seq)\n",
    "        \n",
    "        # create array indicating if position is N\n",
    "        is_N = [True if x == 'N' else False for x in genome_seq]\n",
    "\n",
    "        # create array indicating if position overlaps with a peak\n",
    "        # simultaneously write ATAC-seq peaks\n",
    "        is_peak = [False for x in genome_seq]\n",
    "        current_peak_frame = peak_frame[(peak_frame['chr'] == current_chromosome) &\n",
    "                                        (peak_frame['Factors'].str.contains('Veh'))] # only work with vehicle peaks for now\n",
    "        print(current_chromosome, current_peak_frame.shape)\n",
    "        positions = list(zip(current_peak_frame['ID'].values, current_peak_frame['start'].values, current_peak_frame['end'].values))\n",
    "        for pos in positions:\n",
    "            \n",
    "            ID = pos[0]\n",
    "            start = pos[1] - 1\n",
    "            end = pos[2]\n",
    "            for i in range(start, end):\n",
    "                is_peak[i] = True\n",
    "            tile_peak_file.write('\\t'.join([ID, current_chromosome, str(start), str(end), '+\\n']))  \n",
    "            current_tile_peak_file.write('\\t'.join([ID, current_chromosome, str(start), str(end), '+\\n']))  \n",
    "        \n",
    "        # write background peak files\n",
    "        for i in range(0,genome_seq_size, peakSize):\n",
    "            n_count = np.sum(is_N[i:i+peakSize+1])\n",
    "            if not n_count > peakSize * N_threshold:\n",
    "                if not np.sum(is_peak[i:i+peakSize+1]) > 0:\n",
    "                    tile_peak_file.write('\\t'.join(['tile_'+str(i), current_chromosome, str(i), str(i+peakSize), '+\\n']))        \n",
    "                    current_tile_peak_file.write('\\t'.join([current_chromosome + '_tile_'+str(i), current_chromosome, str(i), str(i+peakSize), '+\\n']))        \n",
    "\n",
    "        current_tile_peak_file.close()\n",
    "tile_peak_file.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "peakDirectory =  './group_by_chromosome/'\n",
    "for chrom in os.listdir(peakDirectory):\n",
    "    peakPath = peakDirectory + chrom + '/' + chrom + '_tile_peaks.tsv'\n",
    "    seqPath = peakPath.replace('_peaks.tsv','.fa')\n",
    "    !homerTools extract $peakPath /bioinformatics/homer/data/genomes/mm10 -fa > $seqPath\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a script to scan for motifs using FIMO\n",
    "! if [ ! -d ./fimo_results/ ]; then mkdir ./fimo_results/; fi\n",
    "! if [ ! -d ./fimo_out/ ]; then mkdir ./fimo_out/; fi\n",
    "\n",
    "\n",
    "pthresh = 0.01\n",
    "motif_dir = '/home/jenhan/analysis/cobinding_motif_analysis/fimo_motifs/'\n",
    "fimo_results_dir = './fimo_results'\n",
    "\n",
    "\n",
    "peakDirectory =  './group_by_chromosome/'\n",
    "for chrom in os.listdir(peakDirectory):\n",
    "    scriptFile = open('scanMotifs_' + chrom + '.sh','w')\n",
    "    for m in os.listdir(motif_dir):\n",
    "        fimo_out_dir = './fimo_out/' + chrom + '_' + m.replace('.fimo','')\n",
    "\n",
    "        if 'fimo' in m:\n",
    "            outPath = fimo_results_dir + '/' +chrom + '_'+ m.replace('.fimo','') +'.txt'\n",
    "            scriptFile.write(\n",
    "                'fimo --text --max-stored-scores 2000000 --output-pthresh ' + \n",
    "                str(pthresh)  + ' ' +\n",
    "                motif_dir + '/' + m + ' ./group_by_chromosome/' + chrom + '/' + chrom + '_tile.fa ' +\n",
    "                '> ' + outPath + ' & \\n')\n",
    "    scriptFile.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "chmod a+x ./scanMotifs*.sh\n",
    "for i in ./scanMotifs*sh; \n",
    "    do echo 'sleeping...';\n",
    "    echo $i;\n",
    "    $i;\n",
    "    sleep 5m;\n",
    "done\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
