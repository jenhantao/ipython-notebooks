{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Background Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: PATH=/gpfs/data01/glasslab/home/jtao/perl5/bin:/gpfs/data01/glasslab/home/jtao/software/anaconda3/bin:/home/jtao/software/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/usr/local/bin:/usr/bin:/gpfs/data01/glasslab/home/jtao/software/homer/bin:/gpfs/data01/glasslab/home/jtao/software/weblogo:/home/jtao/code/seq_merge_pipe:/home/vlink/mouse_strains/marge/shifting:/bioinformatics/glassutils/scripts:/bioinformatics/software/meme/bin:/home/jtao/software/lsgkm/bin\n"
     ]
    }
   ],
   "source": [
    "### header ###\n",
    "__author__ = \"Jenhan Tao\"\n",
    "__license__ = \"BSD\"\n",
    "__email__ = \"jenhantao@gmail.com\"\n",
    "\n",
    "### imports ###\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib\n",
    "import itertools\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "import matplotlib_venn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from random import shuffle\n",
    "\n",
    "### notebook specific configuration ###\n",
    "%matplotlib inline\n",
    "matplotlib.pylab.rcParams['savefig.dpi'] = 200\n",
    "sys.setrecursionlimit(5000)\n",
    "os.chdir('/gpfs/data01/glasslab/home/jtao/analysis/random_background_analysis/')\n",
    "sns.set_context('notebook')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env PATH=/gpfs/data01/glasslab/home/jtao/perl5/bin:/gpfs/data01/glasslab/home/jtao/software/anaconda3/bin:/home/jtao/software/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/usr/local/bin:/usr/bin:/gpfs/data01/glasslab/home/jtao/software/homer/bin:/gpfs/data01/glasslab/home/jtao/software/weblogo:/home/jtao/code/seq_merge_pipe:/home/vlink/mouse_strains/marge/shifting:/bioinformatics/glassutils/scripts:/bioinformatics/software/meme/bin:/home/jtao/software/lsgkm/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into GC content matched training and test data\n",
    "def get_GC_matched_split(features, labels, test_size, tolerance = 0.01):\n",
    "    '''\n",
    "    feature: 2D array (samples x features)\n",
    "    labels: 1D boolean array (samples x)\n",
    "    test_size: fraction of data to test on\n",
    "    tolerance: max difference in GC content between True and False labelled samples\n",
    "    '''\n",
    "    global _id_sequence_dict\n",
    "    \n",
    "    ### match GC content of samples labelled True with those labelled False by thowing out False samples\n",
    "    # retrieve sequences using index of labels\n",
    "    index_label_tuples = tuple(zip(labels.index.values, labels.values))\n",
    "    \n",
    "    true_sequences = [_id_sequence_dict[x[0]] for x in index_label_tuples if x[1]]\n",
    "    true_ids = [x[0] for x in index_label_tuples if x[1]]\n",
    "    \n",
    "    false_sequences = [_id_sequence_dict[x[0]] for x in index_label_tuples if not x[1]]\n",
    "    false_ids = [x[0] for x in index_label_tuples if not x[1]]\n",
    "    \n",
    "    # calculate GC content of True samples\n",
    "    true_gc_count = 0\n",
    "    true_length = 0\n",
    "    for s in true_sequences:\n",
    "        true_gc_count += s.count('G')\n",
    "        true_gc_count += s.count('C')\n",
    "        true_length += len(s)\n",
    "    true_gc_content = true_gc_count/(true_length+0.0000001)\n",
    "    \n",
    "    # calcuate GC content of False samples\n",
    "    false_gc_count = 0\n",
    "    false_length = 0\n",
    "    for s in false_sequences:\n",
    "        false_gc_count += s.count('G')\n",
    "        false_gc_count += s.count('C')\n",
    "        false_length += len(s)\n",
    "    false_gc_content = false_gc_count/(false_length+0.0000001)\n",
    "    \n",
    "    while abs(true_gc_content - false_gc_content) > tolerance:\n",
    "        # remove false GC sequences until GC content matches tolerance\n",
    "        selected_seq = False\n",
    "        \n",
    "        while not selected_seq:\n",
    "            rand_index = np.random.randint(len(false_sequences))\n",
    "            current_seq = false_sequences[rand_index]\n",
    "            current_gc_count = current_seq.count('G')+ current_seq.count('C')\n",
    "            current_length = len(current_seq)\n",
    "            current_gc = current_gc_count/current_length\n",
    "            if true_gc_content > false_gc_content:\n",
    "                # remove sequences that would increase overall GC content of False sequences\n",
    "                if current_gc < false_gc_content:\n",
    "                    selected_seq = True\n",
    "            else:\n",
    "                # remove sequences that would decrease overall GC content of False sequences\n",
    "                if current_gc > false_gc_content:\n",
    "                    selected_seq = True\n",
    "        false_gc_count -= current_gc_count\n",
    "        false_length -= current_length\n",
    "        false_gc_content = false_gc_count/false_length\n",
    "        \n",
    "        false_sequences.pop(rand_index)\n",
    "        false_ids.pop(rand_index)\n",
    "    \n",
    "    filtered_ids = true_ids + false_ids\n",
    "    filtered_features = features[features.index.isin(filtered_ids)]\n",
    "    filtered_labels = labels[labels.index.isin(filtered_ids)]\n",
    "\n",
    "    if test_size <= 0.5:\n",
    "        training_indices, test_indices = next(iter(\n",
    "                sklearn.cross_validation.StratifiedKFold(filtered_labels, int(1/test_size), shuffle=True)))\n",
    "    else:\n",
    "        test_indices, training_indices = next(\n",
    "            iter(sklearn.cross_validation.StratifiedKFold(filtered_labels, int(1/(1-test_size)), shuffle=True)))\n",
    "    training_ids = [filtered_ids[i] for i in training_indices]\n",
    "    test_ids = [filtered_ids[i] for i in test_indices]\n",
    "    \n",
    "    training_features = filtered_features[filtered_features.index.isin(training_ids)]\n",
    "    test_features = filtered_features[filtered_features.index.isin(test_ids)]\n",
    "    training_labels = filtered_labels[filtered_labels.index.isin(training_ids)]\n",
    "    test_labels = filtered_labels[filtered_labels.index.isin(test_ids)]\n",
    "    \n",
    "    return training_features, test_features, training_labels, test_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into GC content matched training and test data\n",
    "def get_split(features, labels, test_size):\n",
    "    '''\n",
    "    feature: 2D array (samples x features)\n",
    "    labels: 1D boolean array (samples x)\n",
    "    test_size: fraction of data to test on\n",
    "    '''\n",
    "    \n",
    "    ### match GC content of samples labelled True with those labelled False by thowing out False samples\n",
    "    # retrieve sequences using index of labels\n",
    "    index_label_tuples = tuple(zip(labels.index.values, labels.values))\n",
    "    \n",
    "    true_ids = [x[0] for x in index_label_tuples if x[1]]\n",
    "    \n",
    "    false_ids = [x[0] for x in index_label_tuples if not x[1]]\n",
    "       \n",
    "    filtered_ids = true_ids + false_ids\n",
    "    filtered_features = features[features.index.isin(filtered_ids)]\n",
    "    filtered_labels = labels[labels.index.isin(filtered_ids)]\n",
    "\n",
    "    if test_size <= 0.5:\n",
    "        training_indices, test_indices = next(iter(\n",
    "                sklearn.cross_validation.StratifiedKFold(filtered_labels, int(1/test_size), shuffle=True)))\n",
    "    else:\n",
    "        test_indices, training_indices = next(\n",
    "            iter(sklearn.cross_validation.StratifiedKFold(filtered_labels, int(1/(1-test_size)), shuffle=True)))\n",
    "    training_ids = [filtered_ids[i] for i in training_indices]\n",
    "    test_ids = [filtered_ids[i] for i in test_indices]\n",
    "    \n",
    "    training_features = filtered_features[filtered_features.index.isin(training_ids)]\n",
    "    test_features = filtered_features[filtered_features.index.isin(test_ids)]\n",
    "    training_labels = filtered_labels[filtered_labels.index.isin(training_ids)]\n",
    "    test_labels = filtered_labels[filtered_labels.index.isin(test_ids)]\n",
    "    \n",
    "    return training_features, test_features, training_labels, test_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Score Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_score_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_sequence_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_strand_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_start_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_end_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/summary_frame.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/annotation_frame.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/peak_sequences/C57BL6J.fa ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Score Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "motif_score_frame=pd.read_pickle('motif_score_frame_C57BL6J.pickle')\n",
    "motif_sequence_frame = pd.read_pickle('motif_sequence_frame_C57BL6J.pickle')\n",
    "motif_strand_frame = pd.read_pickle('motif_strand_frame_C57BL6J.pickle')\n",
    "motif_start_frame = pd.read_pickle('motif_start_frame_C57BL6J.pickle')\n",
    "motif_end_frame = pd.read_pickle('motif_end_frame_C57BL6J.pickle')\n",
    "summary_frame = pd.read_pickle('summary_frame.pickle')\n",
    "annotation_frame = pd.read_pickle('annotation_frame.pickle')\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "normed_motif_frame = pd.DataFrame(scaler.fit_transform(motif_score_frame.ix[:,3:]))\n",
    "normed_motif_frame.columns = motif_score_frame.columns.values[3:]\n",
    "normed_motif_frame.index = motif_score_frame.index.values\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "standardized_motif_frame = pd.DataFrame(scaler.fit_transform(motif_score_frame.ix[:,3:]))\n",
    "standardized_motif_frame.columns = motif_score_frame.columns.values[3:]\n",
    "standardized_motif_frame.index = motif_score_frame.index.values\n",
    "\n",
    "_factors = sorted(list(set([x.split('_')[1] for x in summary_frame.columns if '_' in x])))\n",
    "_factors.remove('atac')\n",
    "\n",
    "### read in sequences as dictionary {peakID: sequence}\n",
    "with open('./C57BL6J.fa') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "_id_sequence_dict = {}\n",
    "for line in data:\n",
    "    if line[0] == '>':\n",
    "        sequenceName = line.strip()[1:]\n",
    "    else:\n",
    "        _id_sequence_dict[sequenceName] = line.strip().upper()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create background peaks from genomic sequences from each chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def getRandomBackgroundSwitch(target_positions, \n",
    "#                         size_ratio = 1.0, \n",
    "#                         tolerance = 0.05, \n",
    "#                         N_threshold = 0.5 ):\n",
    "#     '''\n",
    "#     target_sequences: 2D numpy array, list of genomic coordinates for target sequences [[chr,start,end],...]\n",
    "#     size_ratio: float, number of background sequences relative to target sequences\n",
    "#     tolerance: float, max difference in GC content between True and background labelled samples\n",
    "#     *** Uses mm10 genome taken from Homer ***\n",
    "#     '''\n",
    "    \n",
    "#     ###load mm10 genome into memory\n",
    "    \n",
    "#     # index target positions\n",
    "#     # {chr:[]}, value is chromosome length boolean array\n",
    "#     # largest chromosome has 200 million bps \n",
    "#     _chromosomes = ['chr1' , 'chr2' , 'chr3' , 'chr4' , 'chr5' , \n",
    "#                     'chr6' , 'chr7' , 'chr8' , 'chr9' , 'chr10', \n",
    "#                     'chr11', 'chr12', 'chr13', 'chr14', 'chr15', \n",
    "#                     'chr16', 'chr17', 'chr18', 'chr19', 'chrX']\n",
    "#     _chrom_size_dict = {}\n",
    "#     _chrom_seq_dict = {}\n",
    "#     for chrom in _chromosomes:\n",
    "#         with open('./mm10_genome/' + chrom + '.fa') as f:\n",
    "#             data = f.readlines()\n",
    "#         seq = ''.join(x.upper().strip() for x in data[1:])\n",
    "#         size = len(seq)\n",
    "#         _chrom_size_dict[chrom] = size\n",
    "#         _chrom_seq_dict[chrom] = seq\n",
    "#     _numChromosomes = len(_chromosomes)\n",
    "    \n",
    "#     target_chr_position_dict = {x:np.zeros(200000000) for x in _chromosomes} \n",
    "#     ### initialize target_chr_position_dict using target positions\n",
    "#     ### retreive target sequences\n",
    "#     target_sequences = []\n",
    "#     for pos in target_positions:\n",
    "#         chrom = pos[0]        \n",
    "#         start = pos[1]\n",
    "#         end = pos[2]\n",
    "#         target_chr_position_dict[chrom][start-1:end] = 1 # use 0 indexing of position, versus 1 indexing used in fasta\n",
    "#         seq = _chrom_seq_dict[chrom][start:(end+1)]\n",
    "#         target_sequences.append(seq)\n",
    "#     ### calculate GC content and average length of the target sequences\n",
    "#     target_gc_count = 0\n",
    "#     target_length_count = 0\n",
    "#     for s in target_sequences:\n",
    "#         target_gc_count += s.count('G')\n",
    "#         target_gc_count += s.count('C')\n",
    "#         target_length_count += len(s)\n",
    "#     target_gc_content = target_gc_count/(target_length_count+0.0000001) # GC content of target sequences\n",
    "#     mean_target_length = target_length_count/len(target_sequences) # average length of target sequences\n",
    "#     mean_target_length = int(mean_target_length)\n",
    "    \n",
    "#     ### select random genomic loci such that they do no overlap target sequences\n",
    "#     numSelected = 0\n",
    "#     numToSelect = len(target_positions) * size_ratio * 2 # candidate pool of background seqs is 2X larger\n",
    "#     candidate_positions = []\n",
    "#     while numSelected < numToSelect:\n",
    "#         # select random chromsome\n",
    "#         chromIndex = np.random.randint(_numChromosomes)\n",
    "#         randChrom = _chromosomes[chromIndex]\n",
    "#         randChromSize = _chrom_size_dict[randChrom]\n",
    "#         # must find non overlapping segment on this chromosome before moving on\n",
    "#         selectedSequence = False\n",
    "#         while not selectedSequence:\n",
    "#             randStart = np.random.randint(randChromSize)\n",
    "#             randEnd = randStart + mean_target_length\n",
    "#             overlap_sum = np.sum(target_chr_position_dict[randChrom][randStart:(randEnd + 1)])\n",
    "#             if not overlap_sum > 0:\n",
    "#                 selectedSequence = True\n",
    "#                 numSelected+=1\n",
    "#                 candidate_positions.append([randChrom, randStart, randEnd])\n",
    "\n",
    "#     ### retrieve sequences of random genomic loci\n",
    "#     numFiltered=0\n",
    "#     filtered_candidate_positions = []\n",
    "#     numNallowed = int(N_threshold * mean_target_length)\n",
    "#     for cp in candidate_positions:\n",
    "#         chrom = cp[0]\n",
    "#         start = cp[1]\n",
    "#         end = cp[2]\n",
    "#         candidate_seq = _chrom_seq_dict[chrom][start:(end+1)]\n",
    "#         numN = candidate_seq.count('N')\n",
    "#         # throw away background peaks containing greater than this fraction of N\n",
    "#         if numN <= numNallowed:\n",
    "#             filtered_candidate_positions.append((chrom, start, end,candidate_seq))\n",
    "\n",
    "#     if len(filtered_candidate_positions) < len(target_positions):\n",
    "#         print('The genome is vast and empty and filled with Ns')\n",
    "#         return None\n",
    "       \n",
    "#     ### select random set of candidate background loci\n",
    "#     random.shuffle(filtered_candidate_positions)\n",
    "    \n",
    "#     toReturn_positions = filtered_candidate_positions[:len(target_positions)]\n",
    "#     remaining_positions = filtered_candidate_positions[len(target_positions):]\n",
    "#     # calcuate GC content of background samples\n",
    "#     background_gc_count = 0\n",
    "#     background_length = 0\n",
    "#     for trp in toReturn_positions:\n",
    "#         s = trp[3]\n",
    "#         background_gc_count += s.count('G')\n",
    "#         background_gc_count += s.count('C')\n",
    "#         background_length += len(s)\n",
    "#     background_gc_content = background_gc_count/(background_length+0.0000001)\n",
    "        \n",
    "#     numToReturn = len(toReturn_positions)\n",
    "#     numRemaining = len(remaining_positions)\n",
    "#     counter = 0\n",
    "#     while abs(target_gc_content - background_gc_content) > tolerance:\n",
    "#         # swith background GC sequences until GC content matches tolerance\n",
    "#         switched_seq = False       \n",
    "#         while not switched_seq:\n",
    "#             # sequence to be switched out\n",
    "#             rand_index = np.random.randint(numToReturn)\n",
    "#             current_seq = toReturn_positions[rand_index][3]\n",
    "#             current_gc_count = current_seq.count('G')+ current_seq.count('C')\n",
    "#             current_length = len(current_seq)\n",
    "#             current_gc = current_gc_count/current_length\n",
    "            \n",
    "#             # sequence to be switched out\n",
    "#             switch_index = np.random.randint(numRemaining)\n",
    "#             switch_seq = remaining_positions[switch_index][3]\n",
    "#             switch_gc_count = switch_seq.count('G')+ switch_seq.count('C')\n",
    "#             switch_length = len(switch_seq)\n",
    "#             switch_gc = switch_gc_count/switch_length\n",
    "#             if target_gc_content > background_gc_content:\n",
    "#                 # switch sequences that would increase overall GC content of background sequences\n",
    "#                 if switch_gc > current_gc:\n",
    "#                     switched_seq = True\n",
    "#             else:\n",
    "#                 # switch sequences that would decrease overall GC content of background sequences\n",
    "#                 if switch_gc < current_gc:\n",
    "#                     switched_seq = True\n",
    "#         counter +=1\n",
    "#         if counter % 1000 == 0:\n",
    "#             print(background_gc_content, target_gc_content, tolerance)\n",
    "#         # switch sequences\n",
    "#         temp_pos = toReturn_positions[rand_index]\n",
    "#         toReturn_positions[rand_index] = remaining_positions[switch_index]\n",
    "#         remaining_positions[switch_index] = temp_pos\n",
    "\n",
    "#         # update background GC content\n",
    "#         background_gc_count -= current_gc_count\n",
    "#         background_length -= current_length\n",
    "#         background_gc_count += switch_gc_count\n",
    "#         background_length += switch_length\n",
    "        \n",
    "#         background_gc_content = background_gc_count/background_length\n",
    "          \n",
    "        \n",
    "#     print(target_gc_content, background_gc_content)\n",
    "#     return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRandomBackground(target_positions, \n",
    "                        size_ratio = 1.0, \n",
    "                        tolerance = 0.01, \n",
    "                        N_threshold = 0.5 ):\n",
    "    '''\n",
    "    target_sequences: 2D numpy array, list of genomic coordinates for target sequences [[chr,start,end],...]\n",
    "    size_ratio: float, number of background sequences relative to target sequences\n",
    "    tolerance: float, max difference in GC content between True and background labelled samples\n",
    "    *** Uses mm10 genome taken from Homer ***\n",
    "    '''\n",
    "    \n",
    "    ###load mm10 genome into memory\n",
    "    \n",
    "    # index target positions\n",
    "    # {chr:[]}, value is chromosome length boolean array\n",
    "    # largest chromosome has 200 million bps \n",
    "    _chromosomes = ['chr1' , 'chr2' , 'chr3' , 'chr4' , 'chr5' , \n",
    "                    'chr6' , 'chr7' , 'chr8' , 'chr9' , 'chr10', \n",
    "                    'chr11', 'chr12', 'chr13', 'chr14', 'chr15', \n",
    "                    'chr16', 'chr17', 'chr18', 'chr19', 'chrX']\n",
    "    _chrom_size_dict = {}\n",
    "    _chrom_seq_dict = {}\n",
    "    for chrom in _chromosomes:\n",
    "        with open('./mm10_genome/' + chrom + '.fa') as f:\n",
    "            data = f.readlines()\n",
    "        seq = ''.join(x.upper().strip() for x in data[1:])\n",
    "        size = len(seq)\n",
    "        _chrom_size_dict[chrom] = size\n",
    "        _chrom_seq_dict[chrom] = seq\n",
    "    _numChromosomes = len(_chromosomes)\n",
    "    \n",
    "    target_chr_position_dict = {x:np.zeros(200000000) for x in _chromosomes} \n",
    "    ### initialize target_chr_position_dict using target positions\n",
    "    ### retreive target sequences\n",
    "    target_sequences = []\n",
    "    for pos in target_positions:\n",
    "        chrom = pos[0]        \n",
    "        start = pos[1]\n",
    "        end = pos[2]\n",
    "        target_chr_position_dict[chrom][start-1:end] = 1 # use 0 indexing of position, versus 1 indexing used in fasta\n",
    "        seq = _chrom_seq_dict[chrom][start:(end+1)]\n",
    "        target_sequences.append(seq)\n",
    "    ### calculate GC content and average length of the target sequences\n",
    "    target_gc_count = 0\n",
    "    target_length_count = 0\n",
    "    for s in target_sequences:\n",
    "        target_gc_count += s.count('G')\n",
    "        target_gc_count += s.count('C')\n",
    "        target_length_count += len(s)\n",
    "    target_gc_content = target_gc_count/(target_length_count+0.0000001) # GC content of target sequences\n",
    "    mean_target_length = target_length_count/len(target_sequences) # average length of target sequences\n",
    "    mean_target_length = int(mean_target_length)\n",
    "    \n",
    "    ### select random genomic loci such that they do no overlap target sequences\n",
    "    numSelected = 0\n",
    "    numToSelect = len(target_positions) * size_ratio * 2 # candidate pool of background seqs is 2X larger\n",
    "    candidate_positions = []\n",
    "    numNallowed = int(N_threshold * mean_target_length) # number of allowable Ns\n",
    "    counter = 0\n",
    "    while numSelected < numToSelect:\n",
    "        if counter % 100000 == 0:\n",
    "            print(counter, numSelected)\n",
    "        # select random chromsome\n",
    "        chromIndex = np.random.randint(_numChromosomes)\n",
    "        randChrom = _chromosomes[chromIndex]\n",
    "        randChromSize = _chrom_size_dict[randChrom]\n",
    "        # must find non overlapping segment on this chromosome before moving on\n",
    "        selectedSequence = False\n",
    "        while not selectedSequence:\n",
    "            counter += 1\n",
    "            randStart = np.random.randint(randChromSize)\n",
    "            randEnd = randStart + mean_target_length\n",
    "            overlap_sum = np.sum(target_chr_position_dict[randChrom][randStart:(randEnd + 1)])\n",
    "            \n",
    "            if not overlap_sum > 0:\n",
    "                randSeq = _chrom_seq_dict[randChrom][randStart:(randEnd+1)]\n",
    "                numN = randSeq.count('N')\n",
    "                if numN <= numNallowed:\n",
    "                    rand_gc_count = randSeq.count('G')+ randSeq.count('C')\n",
    "                    rand_gc = rand_gc_count/mean_target_length\n",
    "                    if abs(target_gc_content - rand_gc) <= tolerance:\n",
    "                        selectedSequence = True\n",
    "                        numSelected+=1\n",
    "                        candidate_positions.append([randChrom, randStart, randEnd, randSeq])\n",
    "    # calcuate GC content of background samples\n",
    "    background_gc_count = 0\n",
    "    background_length = 0\n",
    "    for cp in candidate_positions:\n",
    "        s = cp[3]\n",
    "        background_gc_count += s.count('G')\n",
    "        background_gc_count += s.count('C')\n",
    "        background_length += len(s)\n",
    "    background_gc_content = background_gc_count/(background_length+0.0000001)\n",
    "    print(target_gc_content,background_gc_content)\n",
    "    return candidate_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate random genomic background for all monomers\n",
    "strain = 'c57bl6'\n",
    "for treatment in ['veh', 'kla']:\n",
    "    for monomer in ap1_members:\n",
    "        target_indices = summary_frame[summary_frame[strain + '_' + monomer + '_' + treatment] > 0.0].index.values\n",
    "        target_positions = summary_frame[summary_frame.index.isin(target_indices)][['chr', 'start', 'end']].values\n",
    "        start = time.time()\n",
    "        backgroundPositions = getRandomBackground(target_positions, N_threshold =1.0, tolerance=0.05, size_ratio=5)\n",
    "        end = time.time()\n",
    "        print(monomer, treatment, end - start)\n",
    "        pickle.dump(backgroundPositions,open('./background_pickles/' + monomer + '_' + treatment + '_background.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background positions to create peak files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strain = 'c57bl6'\n",
    "! if [ ! -d ./background_peak_files ]; then mkdir ./background_peak_files; fi\n",
    "for treatment in ['veh', 'kla']:\n",
    "    for monomer in ap1_members:\n",
    "        backgroundPositions = pickle.load(open('./background_pickles/' + monomer + '_' + treatment + '_background.pickle', 'rb'))\n",
    "        outFile = open('./background_peak_files/' + strain + '_' + monomer + '_' + treatment + '-background_peaks.tsv' , 'w')\n",
    "        outFile.write('\\t'.join(['#PeakID','chr','start','end','strand','idrScore', 'count','\\n']))\n",
    "        counter = 0\n",
    "        for pos in backgroundPositions:\n",
    "            chrom = pos[0]\n",
    "            start = str(pos[1])\n",
    "            end = str(pos[2])\n",
    "            strand = '+' # arbitrary - for compatibility with downstream scripts\n",
    "            score = '1' # arbitrary - for compatibility with downstream scripts\n",
    "            randID = 'background_' + str(np.random.randint(10000)) + '_' + str(counter)\n",
    "            counter += 1\n",
    "            outFile.write('\\t'.join([randID, chrom, start, end, strand, score, score, '\\n']))\n",
    "        outFile.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Peak Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! mergePeaks -d given ./background_peak_files/*tsv > ./background_merged_peaks.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys:1: DtypeWarning: Columns (8,9,10,11,12,13,14,15,16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "Finished reading merged peak file...\n",
      "Integrating scores for c57bl6_atf3_kla-background\n",
      "Integrating scores for c57bl6_atf3_veh-background\n",
      "Integrating scores for c57bl6_cjun_kla-background\n",
      "Integrating scores for c57bl6_cjun_veh-background\n",
      "Integrating scores for c57bl6_fos_kla-background\n",
      "Integrating scores for c57bl6_fos_veh-background\n",
      "Integrating scores for c57bl6_junb_kla-background\n",
      "Integrating scores for c57bl6_junb_veh-background\n",
      "Integrating scores for c57bl6_jund_kla-background\n",
      "Integrating scores for c57bl6_jund_veh-background\n"
     ]
    }
   ],
   "source": [
    "! makeSummaryFile.py ./background_merged_peaks.tsv ./background_group_summary.tsv ./background_peak_files/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read in peak data data\n",
    "summary_background_frame = pd.read_csv('./background_group_summary.tsv' , sep='\\t')\n",
    "summary_background_frame = background_summary.fillna('0')\n",
    "for col in summary_background_frame.columns[5:]:\n",
    "    floatValues = []\n",
    "    for val in summary_background_frame[col].values.astype(str):\n",
    "        if ',' in val:\n",
    "            maxVal = np.mean([float(x) for x in val.split(',')])\n",
    "            floatValues.append(maxVal)\n",
    "        else:\n",
    "            floatValues.append(float(val))\n",
    "    summary_background_frame[col] = floatValues\n",
    "summary_background_frame.index = background_summary['ID'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a script to scan for motifs using FIMO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d ./peak_sequences ] ;\n",
    "    then mkdir ./peak_sequences\n",
    "else\n",
    "    rm ./peak_sequences/*\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving peaks\n",
      "Loading shift vectors\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "perl /home/vlink/mouse_strains/marge/analysis/extract_seq_from_peakfiles.pl -strains C57BL6J -file ./background_merged_peaks.tsv -output ./peak_sequences/C57BL6J_marge.fa\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reformat fastq files to use homer peak IDs\n",
    "\n",
    "coordinate_peakID_dict = {} # {chr_start_end:homerID}\n",
    "with open ('./background_merged_peaks.tsv') as f:\n",
    "    data = f.readlines()\n",
    "for line in data[1:]:\n",
    "    tokens = line.split('\\t')\n",
    "    coordinate = '_'.join(tokens[1:4])\n",
    "    peakID = tokens[0].strip()\n",
    "    coordinate_peakID_dict[coordinate] = tokens[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C57BL6J_marge.fa\n"
     ]
    }
   ],
   "source": [
    "for fastaFile in os.listdir('./peak_sequences/'):\n",
    "    if 'marge' in fastaFile:\n",
    "        strain = fastaFile.split('_')[0]\n",
    "        outFile = open('./peak_sequences/' + fastaFile.replace('_marge',''), 'w')\n",
    "        print(fastaFile)\n",
    "        with open('./peak_sequences/' + fastaFile) as f:\n",
    "            data = f.readlines()\n",
    "        for line in data:\n",
    "            if '>' in line:\n",
    "                coordinate = line[1:].replace('_'+strain,'').strip()\n",
    "                \n",
    "                peakID = coordinate_peakID_dict[coordinate]\n",
    "                outFile.write('>' + peakID + '\\n')\n",
    "               \n",
    "            else:\n",
    "                outFile.write(line)\n",
    "        outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C57BL6J.fa\n"
     ]
    }
   ],
   "source": [
    "# create a script to scan for motifs using FIMO\n",
    "! if [ ! -d /home/jtao/analysis/random_background_analysis/fimo_results/ ]; then mkdir /home/jtao/analysis/random_background_analysis/fimo_results/; fi\n",
    "! if [ ! -d /home/jtao/analysis/random_background_analysis/fimo_out/ ]; then mkdir /home/jtao/analysis/random_background_analysis/fimo_out/; fi\n",
    "! rm -rf ./fimo_out/*\n",
    "! rm -rf ./fimo_result/*\n",
    "\n",
    "\n",
    "pthresh = 0.01\n",
    "motif_dir = '/home/jtao/analysis/cobinding_motif_analysis/fimo_motifs/'\n",
    "\n",
    "fimo_results_dir = './fimo_results'\n",
    "\n",
    "for fastaFile in os.listdir('./peak_sequences/'):\n",
    "    if not 'marge' in fastaFile:\n",
    "        print(fastaFile)\n",
    "        strain = fastaFile.split('.')[0]\n",
    "        count = 0\n",
    "        scriptFile = open('scanMotifs_background_'+ strain +'.sh','w')\n",
    "        for m in os.listdir(motif_dir):\n",
    "            if 'fimo' in m:\n",
    "                fimo_out_dir = './fimo_out/' + strain + '_' +m.replace('.fimo','')\n",
    "                outPath = fimo_results_dir + '/' +strain + '_' + m.replace('.fimo','') +'.txt'\n",
    "                scriptFile.write(\n",
    "                    '(sleep ' + str(15 * count) + \n",
    "                    's; fimo --text --max-stored-scores 2000000 --output-pthresh ' + \n",
    "                    str(pthresh) +' --oc ' + fimo_out_dir + ' ' +\n",
    "                    motif_dir + '/' + m + ' ./peak_sequences/' + fastaFile +\n",
    "                    '> ' + outPath + ' ) & \\n')\n",
    "                count+=1\n",
    "        scriptFile.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "chmod a+x ./scanMotifs*.sh\n",
    "for i in ./scanMotifs*sh; \n",
    "    do echo 'sleeping...';\n",
    "    echo $i;\n",
    "    $i;\n",
    "#     sleep 5m;\n",
    "done\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Motif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @jit(nopython=False)\n",
    "# def readMotifScores(fimoPaths, all_peak_ids):\n",
    "#     '''\n",
    "#     merges FIMO outputs for many motifs into a single data object\n",
    "#     inputs: list of file paths to FIMO results\n",
    "#     '''\n",
    "    \n",
    "#     ### initialize data structures for storing output\n",
    "#     for fimo_result_path in fimoPaths:\n",
    "#         toReturn = []\n",
    "#         # read in fimo result file\n",
    "#         motif_name = fimo_result_path.split('/')[-1].replace('.fimo','')\n",
    "#         fimo_result_file = open(fimo_result_path, 'r')\n",
    "#         fimo_result_data = fimo_result_file.readlines()\n",
    "#         fimo_result_file.close()\n",
    "        \n",
    "#         # initialize arrays for storing current file\n",
    "#         ids = []\n",
    "#         scores = []\n",
    "#         strands = []\n",
    "#         sequences = []\n",
    "#         starts = []\n",
    "#         ends = []\n",
    "#         fimo_result_data.pop(0) # skip header\n",
    "#         for line in fimo_result_data: \n",
    "#             # break up row into column values\n",
    "#             tokens = line.split('\\t')\n",
    "#             motif_name = tokens[0]\n",
    "#             peak_id = tokens[1]\n",
    "#             start = tokens[2]\n",
    "#             stop = tokens[3]\n",
    "#             strand = tokens[4]\n",
    "#             score = tokens[5]\n",
    "#             pvalue = tokens[6]\n",
    "#             sequence = tokens[7]\n",
    "            \n",
    "#             ids.append(peak_id)\n",
    "#             scores.append(score)\n",
    "#             strands.append(strand)\n",
    "#             sequences.append(sequence)\n",
    "#             starts.append(start)\n",
    "#             ends.append(end)        \n",
    "\n",
    "#         id_count_dict = {} #{PeakID:motifCount}\n",
    "#         id_values_dict = {} # {PeakID:(motifScore, motifSequence, motifStrand, motifStart, motifEnd)}\n",
    "\n",
    "#         for i in range(len(ids)):\n",
    "#             currentScore = float(scores[i])\n",
    "#             currentSequence = sequences[i]\n",
    "#             currentStrand = strands[i]\n",
    "#             currentStart = int(starts[i])\n",
    "#             currentEnd = int(ends[i])\n",
    "\n",
    "#             if currentScore < 0.0:\n",
    "#                 currentScore = 0.0\n",
    "\n",
    "\n",
    "#             values = (currentScore, currentSequence, currentStrand, currentStart, currentEnd)\n",
    "\n",
    "#             if ids[i] in id_values_dict:\n",
    "#                 if currentScore > id_values_dict[ids[i]][0]:\n",
    "#                     id_values_dict[ids[i]] = values\n",
    "#                     id_count_dict[ids[i]] += 1\n",
    "#             else:\n",
    "#                 id_values_dict[ids[i]] = values\n",
    "#                 id_count_dict[ids[i]] = 1\n",
    "                \n",
    "#         # fill in missing values\n",
    "#         sorted_scores = []\n",
    "#         sorted_sequences = []\n",
    "#         sorted_strands = []\n",
    "#         sorted_starts = []\n",
    "#         sorted_ends = []\n",
    "#         sorted_counts = []\n",
    "\n",
    "#         for peak_id in all_peak_ids:\n",
    "#             if peak_id in id_values_dict:\n",
    "#                 current_values = id_values_dict[peak_id]\n",
    "#                 current_score = current_values[0]\n",
    "#                 current_sequence = current_values[1]\n",
    "#                 current_strand = current_values[2]\n",
    "#                 current_start = current_values[3]\n",
    "#                 current_end = current_values[4]\n",
    "#                 if peak_id in id_count_dict:\n",
    "#                     current_count = id_count_dict[peak_id]\n",
    "#                 else:\n",
    "#                     current_count = 0\n",
    "#             else:\n",
    "#                 current_score = ''\n",
    "#                 current_sequence = ''\n",
    "#                 current_strand = '?'\n",
    "#                 current_start = -1\n",
    "#                 current_end = -1\n",
    "#                 current_count = 0\n",
    "#             sorted_scores.append(current_score)\n",
    "#             sorted_sequences = [current_sequence]\n",
    "#             sorted_strands = [current_strand]\n",
    "#             sorted_starts = [current_start]\n",
    "#             sorted_ends = [current_end]\n",
    "#             sorted_counts = [current_count]\n",
    "        \n",
    "\n",
    "#         toReturn.append([sorted_scores,sorted_sequences,sorted_strands,sorted_starts,sorted_ends,sorted_counts])\n",
    "\n",
    "    \n",
    "#     return None\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testPaths  = fimoPaths[:2]\n",
    "all_peak_ids = summary_background_frame.index.values\n",
    "readMotifScores(testPaths, all_peak_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C57BL6J.fa\n",
      "1 alx1_alx4_arx.fimo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ap-1.fimo\n"
     ]
    }
   ],
   "source": [
    "motif_dir = '/home/jtao/analysis/cobinding_motif_analysis/fimo_motifs/'\n",
    "\n",
    "for fastaFile in os.listdir('./peak_sequences/'):\n",
    "    if not 'marge' in fastaFile:\n",
    "        print(fastaFile)\n",
    "        strain = fastaFile.split('.')[0]\n",
    "\n",
    "        peak_start_dict = dict(zip(summary_background_frame['ID'].values, summary_background_frame['start'].values))\n",
    "\n",
    "        motif_score_background_frame = summary_background_frame[['ID', 'Factors', 'chr']]\n",
    "        motif_score_background_frame.index=motif_score_background_frame['ID'].values\n",
    "\n",
    "        motif_sequence_background_frame = summary_background_frame[['ID', 'Factors', 'chr']]\n",
    "        motif_sequence_background_frame.index=motif_score_background_frame['ID'].values\n",
    "\n",
    "        motif_strand_background_frame = summary_background_frame[['ID', 'Factors', 'chr']]\n",
    "        motif_strand_background_frame.index=motif_score_background_frame['ID'].values\n",
    "\n",
    "        motif_start_background_frame = summary_background_frame[['ID', 'Factors', 'chr']]\n",
    "        motif_start_background_frame.index=motif_score_background_frame['ID'].values\n",
    "\n",
    "        motif_end_background_frame = summary_background_frame[['ID', 'Factors', 'chr']]\n",
    "        motif_end_background_frame.index=motif_score_background_frame['ID'].values\n",
    "\n",
    "        motif_count_background_frame = summary_background_frame[['ID', 'Factors', 'chr']]\n",
    "        motif_count_background_frame.index = motif_count_background_frame['ID'].values\n",
    "\n",
    "        counter=0\n",
    "        for m in sorted(os.listdir(motif_dir)):\n",
    "            counter+=1\n",
    "            if '.fimo' in m:\n",
    "                print(counter,m)\n",
    "                motif_results = './fimo_results/' + strain + '_' + m.replace('.fimo','') +'.txt'\n",
    "                fimo_result_background_frame=pd.read_csv(motif_results, \n",
    "                                              skiprows=1,\n",
    "                                              names=['motif_name', \n",
    "                                                     'peak_id', \n",
    "                                                     'start', \n",
    "                                                     'stop', \n",
    "                                                     'strand', \n",
    "                                                     'score', \n",
    "                                                     'pvalue', \n",
    "                                                     'sequence'],\n",
    "                                              sep='\\t')\n",
    "                motif_name = m.replace('.fimo','')\n",
    "\n",
    "                id_count_dict = {} #{PeakID:motifCount}\n",
    "                id_values_dict = {} # {PeakID:(motifScore, motifSequence, motifStrand, motifStart, motifEnd)}\n",
    "\n",
    "                ids = fimo_result_background_frame['peak_id'].values\n",
    "                scores = fimo_result_background_frame['score'].values\n",
    "                strands = fimo_result_background_frame['strand'].values\n",
    "                sequences = fimo_result_background_frame['sequence']\n",
    "                starts = fimo_result_background_frame['start']\n",
    "                ends = fimo_result_background_frame['stop']\n",
    "\n",
    "                for i in range(len(ids)):\n",
    "                    currentScore = float(scores[i])\n",
    "                    currentSequence = sequences[i]\n",
    "                    currentStrand = strands[i]\n",
    "                    currentStart = int(starts[i])\n",
    "                    currentEnd = int(ends[i])\n",
    "\n",
    "                    if currentScore < 0.0:\n",
    "                        currentScore = 0.0\n",
    "                    \n",
    "                    \n",
    "                    values = (currentScore, currentSequence, currentStrand, currentStart, currentEnd)\n",
    "\n",
    "                    if ids[i] in id_values_dict:\n",
    "                        if currentScore > id_values_dict[ids[i]][0]:\n",
    "                            id_values_dict[ids[i]] = values\n",
    "                            id_count_dict[ids[i]] += 1\n",
    "                    else:\n",
    "                        id_values_dict[ids[i]] = values\n",
    "                        id_count_dict[ids[i]] = 1\n",
    "\n",
    "                sorted_values = [id_values_dict[x] if x in id_values_dict else (0,'','?',-1,-1,0) for x in  motif_score_background_frame['ID'].values]\n",
    "                sorted_scores = [x[0] for x in sorted_values]\n",
    "                sorted_sequences = [x[1] for x in sorted_values]\n",
    "                sorted_strands = [x[2] for x in sorted_values]\n",
    "                sorted_starts = [x[3] for x in sorted_values]\n",
    "                sorted_ends = [x[4] for x in sorted_values]\n",
    "                sorted_counts = [id_count_dict[x] if x in id_count_dict else 0 for x in motif_score_background_frame['ID'].values]\n",
    "\n",
    "                motif_score_background_frame[motif_name] = sorted_scores\n",
    "                motif_sequence_background_frame[motif_name] = sorted_sequences\n",
    "                motif_strand_background_frame[motif_name] = sorted_strands\n",
    "                motif_start_background_frame[motif_name] = sorted_starts\n",
    "                motif_end_background_frame[motif_name] = sorted_ends\n",
    "                motif_count_background_frame[motif_name] = sorted_counts\n",
    "\n",
    "\n",
    "        motif_score_background_frame.to_pickle('motif_score_background_frame_'+  strain + '.pickle')\n",
    "        motif_score_background_frame.to_csv('motif_scores_'+  strain + '.tsv', sep='\\t', index=False)\n",
    "\n",
    "        motif_sequence_background_frame.to_pickle('motif_sequence_background_frame_'+  strain + '.pickle')\n",
    "        motif_sequence_background_frame.to_csv('motif_sequence_'+  strain + '.tsv', sep='\\t', index=False)\n",
    "\n",
    "        motif_strand_background_frame.to_pickle('motif_strand_background_frame_'+  strain + '.pickle')\n",
    "        motif_strand_background_frame.to_csv('motif_strand_'+  strain + '.tsv', sep='\\t', index=False)\n",
    "\n",
    "        motif_start_background_frame.to_pickle('motif_start_background_frame_'+  strain + '.pickle')\n",
    "        motif_start_background_frame.to_csv('motif_start_background_frame_'+  strain + '.tsv', sep='\\t', index=False)\n",
    "\n",
    "        motif_end_background_frame.to_pickle('motif_end_background_frame_'+  strain + '.pickle')\n",
    "        motif_end_background_frame.to_csv('motif_end_background_frame_'+  strain + '.tsv', sep='\\t', index=False)\n",
    "\n",
    "        motif_count_background_frame.to_pickle('motif_count_background_frame_'+  strain + '.pickle')\n",
    "        motif_count_background_frame.to_csv('motif_count_background_frame_'+  strain + '.tsv', sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classifier using Open Chromatin Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numIterations = 5\n",
    "ap1_members = ['atf3','cjun', 'fos', 'junb','jund']    \n",
    "test_size = 0.5\n",
    "factors = ['atf3','cjun', 'fos', 'junb','jund', 'atac', 'cebpa', 'pu1', 'p65']\n",
    "# c57bl6_indices = summary_frame[summary_frame['Factors'].str.contains('c57bl6')].index.values  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:49: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atf3_veh roc: 0.83610172069 3.44979507917e-06 precision: 0.710825327053 9.32621330292e-06 numTestPositives: 10969\n",
      "cjun_veh roc: 0.812963450027 1.03597985076e-05 precision: 0.471024807596 5.39031299559e-06 numTestPositives: 6236\n",
      "fos_veh roc: 0.860914986321 3.15643632627e-06 precision: 0.364336559092 6.13557879045e-05 numTestPositives: 992\n",
      "junb_veh roc: 0.672269589368 6.21464145858e-05 precision: 0.0204829660345 2.08456526182e-06 numTestPositives: 263\n",
      "jund_veh roc: 0.806859001358 2.04753834529e-06 precision: 0.563707505366 1.42685798554e-05 numTestPositives: 8885\n",
      "atf3_kla roc: 0.830329033402 3.74700584789e-06 precision: 0.801807736212 5.36772708301e-06 numTestPositives: 17202\n",
      "cjun_kla roc: 0.806956915332 5.27155667865e-07 precision: 0.479906804462 1.24805951856e-05 numTestPositives: 7994\n",
      "fos_kla roc: 0.831760628236 2.70513489743e-06 precision: 0.662119375555 8.4607291233e-06 numTestPositives: 10745\n",
      "junb_kla roc: 0.827412325119 1.60058468563e-06 precision: 0.479888666034 3.36324625181e-05 numTestPositives: 7165\n",
      "jund_kla roc: 0.819205554882 2.87434007272e-06 precision: 0.713033673815 9.413315478e-06 numTestPositives: 15013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for monomers using all motifs\n",
    "strain = 'c57bl6'\n",
    "factor_auc_dict = {}\n",
    "factor_precision_dict = {}\n",
    "factor_coeff_dict = {}\n",
    "factor_prob_dict = {}\n",
    "factor_meanCoeff_dict = {}\n",
    "factor_intercept_dict = {}\n",
    "factor_meanIntercept_dict = {}\n",
    "for treatment in ['veh', 'kla']:\n",
    "    for monomer in ap1_members:\n",
    "        c57bl6_indices = summary_frame[summary_frame[['c57bl6_' + x + '_' + treatment for x in factors]].sum(axis=1) > 0].index.values  \n",
    "        features = standardized_motif_frame[standardized_motif_frame.index.isin(c57bl6_indices)]\n",
    "        labels = summary_frame[summary_frame.index.isin(c57bl6_indices)][strain + '_' + monomer + '_' + treatment] > 0.0\n",
    "        if np.sum(labels) >= 100:\n",
    "            all_aucs = []\n",
    "            all_coefficients = []\n",
    "            all_probs = None\n",
    "            all_precisions = []\n",
    "            all_intercepts = []\n",
    "            for i in range(numIterations):  \n",
    "\n",
    "                # split data into training and test sets\n",
    "                training_features, test_features, training_labels, test_labels = get_GC_matched_split(\n",
    "                    features, labels, test_size = test_size, tolerance = 0.01)\n",
    "\n",
    "                #  Run classifier\n",
    "                lr_classifier = sklearn.linear_model.LogisticRegression(penalty='l1', n_jobs=-1)\n",
    "\n",
    "                lr_classifier.fit(training_features, training_labels)\n",
    "                # retrieve probabilities\n",
    "                probas_lr = lr_classifier.predict_proba(test_features)\n",
    "\n",
    "                # score predictions\n",
    "                current_roc_auc = sklearn.metrics.roc_auc_score(test_labels, probas_lr[:, 1], average = None)\n",
    "                current_precision = sklearn.metrics.average_precision_score(test_labels, probas_lr[:, 1], average = None)\n",
    "\n",
    "                all_aucs.append(current_roc_auc)\n",
    "                all_precisions.append(current_precision)\n",
    "\n",
    "                # score all sequences\n",
    "                probs = lr_classifier.predict_proba(features)[:, 1]\n",
    "\n",
    "                current_coefficients = lr_classifier.coef_.flatten()\n",
    "                all_coefficients.append(current_coefficients)\n",
    "                all_intercepts.append(lr_classifier.intercept_[0])\n",
    "                \n",
    "                if all_probs == None:\n",
    "                    all_probs = probs\n",
    "                else:\n",
    "                    all_probs = all_probs + probs\n",
    "            mean_coefficients = np.mean(all_coefficients, axis=0)\n",
    "            \n",
    "            factor_auc_dict[monomer + '_' + treatment]= all_aucs\n",
    "            factor_precision_dict[monomer + '_' + treatment] = all_precisions\n",
    "            factor_coeff_dict[monomer + '_' + treatment] = all_coefficients\n",
    "            factor_prob_dict[monomer + '_' + treatment] = all_probs\n",
    "            factor_meanCoeff_dict[monomer + '_' + treatment] = mean_coefficients\n",
    "            factor_intercept_dict[monomer + '_' + treatment] = all_intercepts\n",
    "            factor_meanIntercept_dict[monomer + '_' + treatment] = np.mean(all_intercepts)\n",
    "            print(monomer + '_' + treatment,\n",
    "                  'roc:', np.mean(all_aucs), np.var(all_aucs),\n",
    "                  'precision:', np.mean(all_precisions), np.var(all_precisions),  \n",
    "                  'numTestPositives:', np.sum(test_labels)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Logistic Regression using Random Genomic Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "standardized_motif_background_frame = pd.DataFrame(scaler.fit_transform(motif_score_background_frame.ix[:,3:]))\n",
    "standardized_motif_background_frame.columns = motif_score_background_frame.columns.values[3:]\n",
    "standardized_motif_background_frame.index = motif_score_background_frame.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numIterations = 5\n",
    "ap1_members = ['atf3','cjun', 'fos', 'junb','jund']    \n",
    "test_size = 0.5\n",
    "factors = ['atf3','cjun', 'fos', 'junb','jund', 'atac', 'cebpa', 'pu1', 'p65']\n",
    "# c57bl6_indices = summary_frame[summary_frame['Factors'].str.contains('c57bl6')].index.values  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:59: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atf3_veh roc: 0.917402357455 1.44864536324e-06 precision: 0.777096713825 6.70150610575e-06 numTestPositives: 11013\n",
      "cjun_veh roc: 0.935315493846 6.80897403591e-07 precision: 0.827533261605 3.33182618986e-06 numTestPositives: 6335\n",
      "fos_veh roc: 0.946941499214 1.4454350912e-05 precision: 0.83027582052 4.59802544223e-05 numTestPositives: 994\n",
      "junb_veh roc: 0.83173455127 8.50686578456e-05 precision: 0.698923256979 0.000326516124148 numTestPositives: 247\n",
      "jund_veh roc: 0.915915331233 6.30124727043e-07 precision: 0.779518635626 3.20334833347e-06 numTestPositives: 9025\n",
      "atf3_kla roc: 0.913409716568 5.01326610535e-07 precision: 0.727766399839 8.07884603255e-06 numTestPositives: 17327\n",
      "cjun_kla roc: 0.937144708727 8.33440018159e-07 precision: 0.817832503968 4.44897412885e-06 numTestPositives: 8068\n",
      "fos_kla roc: 0.940092995887 3.37132354601e-07 precision: 0.80862130008 2.90010220952e-06 numTestPositives: 10733\n",
      "junb_kla roc: 0.94854589066 5.01028336266e-07 precision: 0.851497590155 4.54380376101e-06 numTestPositives: 7040\n",
      "jund_kla roc: 0.917091384417 1.2871014342e-06 precision: 0.746474822236 1.15357855762e-05 numTestPositives: 14982\n"
     ]
    }
   ],
   "source": [
    "# for monomers using all motifs\n",
    "strain = 'c57bl6'\n",
    "factor_auc_randomBackground_dict = {}\n",
    "factor_precision_randomBackground_dict = {}\n",
    "factor_coeff_randomBackground_dict = {}\n",
    "factor_prob_randomBackground_dict = {}\n",
    "factor_meanCoeff_randomBackground_dict = {}\n",
    "factor_intercept_randomBackground_dict = {}\n",
    "factor_meanIntercept_randomBackground_dict = {}\n",
    "for treatment in ['veh', 'kla']:\n",
    "    for monomer in ap1_members:\n",
    "        target_indices = summary_frame[summary_frame[strain + '_' + monomer + '_' + treatment] > 0.0].index.values\n",
    "        target_features = standardized_motif_frame[standardized_motif_frame.index.isin(target_indices)]\n",
    "        \n",
    "        background_indices = motif_score_background_frame[\n",
    "            motif_score_background_frame['Factors'] == strain + '_' + monomer + '_' + treatment + '-background'].index.values\n",
    "        # select subset of background indices to use\n",
    "        shuffle(background_indices)\n",
    "        background_indices = background_indices[:5*len(target_indices)]\n",
    "        background_features = standardized_motif_background_frame[standardized_motif_background_frame.index.isin(background_indices)]\n",
    "        \n",
    "        # merge target and background features together \n",
    "        features = pd.concat([target_features, background_features])\n",
    "        labels = pd.Series(data = [True] * len(target_indices) + [False] * len(background_indices),\n",
    "                           index = features.index.values)\n",
    "        if np.sum(labels) >= 100:\n",
    "            all_aucs = []\n",
    "            all_coefficients = []\n",
    "            all_probs = None\n",
    "            all_precisions = []\n",
    "            all_intercepts = []\n",
    "            for i in range(numIterations):  \n",
    "\n",
    "                # split data into training and test sets\n",
    "                training_features, test_features, training_labels, test_labels = get_split(\n",
    "                    features, labels, test_size = test_size)\n",
    "\n",
    "                #  Run classifier\n",
    "                lr_classifier = sklearn.linear_model.LogisticRegression(penalty='l1', n_jobs=-1)\n",
    "\n",
    "                lr_classifier.fit(training_features, training_labels)\n",
    "                # retrieve probabilities\n",
    "                probas_lr = lr_classifier.predict_proba(test_features)\n",
    "\n",
    "                # score predictions\n",
    "                current_roc_auc = sklearn.metrics.roc_auc_score(test_labels, probas_lr[:, 1], average = None)\n",
    "                current_precision = sklearn.metrics.average_precision_score(test_labels, probas_lr[:, 1], average = None)\n",
    "\n",
    "                all_aucs.append(current_roc_auc)\n",
    "                all_precisions.append(current_precision)\n",
    "\n",
    "                # score all sequences\n",
    "                probs = lr_classifier.predict_proba(features)[:, 1]\n",
    "\n",
    "                current_coefficients = lr_classifier.coef_.flatten()\n",
    "                all_coefficients.append(current_coefficients)\n",
    "                all_intercepts.append(lr_classifier.intercept_[0])\n",
    "                \n",
    "                if all_probs == None:\n",
    "                    all_probs = probs\n",
    "                else:\n",
    "                    all_probs = all_probs + probs\n",
    "            mean_coefficients = np.mean(all_coefficients, axis=0)\n",
    "            \n",
    "            factor_auc_randomBackground_dict[monomer + '_' + treatment]= all_aucs\n",
    "            factor_precision_randomBackground_dict[monomer + '_' + treatment] = all_precisions\n",
    "            factor_coeff_randomBackground_dict[monomer + '_' + treatment] = all_coefficients\n",
    "            factor_prob_randomBackground_dict[monomer + '_' + treatment] = all_probs\n",
    "            factor_meanCoeff_randomBackground_dict[monomer + '_' + treatment] = mean_coefficients\n",
    "            factor_intercept_randomBackground_dict[monomer + '_' + treatment] = all_intercepts\n",
    "            factor_meanIntercept_randomBackground_dict[monomer + '_' + treatment] = np.mean(all_intercepts)\n",
    "            print(monomer + '_' + treatment,\n",
    "                  'roc:', np.mean(all_aucs), np.var(all_aucs),\n",
    "                  'precision:', np.mean(all_precisions), np.var(all_precisions),  \n",
    "                  'numTestPositives:', np.sum(test_labels)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run gkmSVM against Random Genomic Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monomers = []\n",
    "aucs = []\n",
    "feature_set = []\n",
    "for factor in sorted(factor_auc_dict_lr.keys()):\n",
    "    monomers = monomers + numIterations * [factor]\n",
    "    aucs = aucs + factor_auc_dict_lr[factor]\n",
    "    feature_set = feature_set + numIterations * ['LR All Motifs']\n",
    "    \n",
    "    monomers = monomers + numIterations * [factor]\n",
    "    aucs = aucs + factor_auc_dict_unbalancedgkmSVM[factor]\n",
    "    feature_set = feature_set + numIterations * ['gkmSVM']\n",
    "    \n",
    "data = pd.DataFrame({'Factor':monomers,\n",
    "                     'AUC':aucs,\n",
    "                     'Feature Set': feature_set\n",
    "                     })\n",
    "\n",
    "# vehicle plot\n",
    "\n",
    "\n",
    "for treatment in ['veh', 'kla']:\n",
    "    factor_auc_tuples = [(x, np.mean(factor_auc_dict_lr[x])) for x in factor_auc_dict_lr if treatment in x]\n",
    "    sorted_monomers = [y[0] for y in sorted(factor_auc_tuples, key=lambda x:x[1])]\n",
    "    with sns.axes_style('ticks'):\n",
    "        plottingFrame = sns.factorplot(data = data[data['Factor'].str.contains(treatment)],\n",
    "                                    x='Factor',\n",
    "                                    y='AUC',\n",
    "                                    order = sorted_monomers,\n",
    "                                    palette='Set1',\n",
    "                                    size=4,\n",
    "                                    hue='Feature Set',\n",
    "                                    kind = 'point', \n",
    "                                    markers = '.', s='0.01')\n",
    "        sns.despine()\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel('aucROC')\n",
    "        plt.xlabel('AP-1 Monomer')\n",
    "        plt.ylim(0.5,1)\n",
    "        plt.title('Classifier Performance')\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
