{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Background Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: PATH=/gpfs/data01/glasslab/home/jtao/perl5/bin:/gpfs/data01/glasslab/home/jtao/software/anaconda3/bin:/home/jtao/software/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/usr/local/bin:/usr/bin:/gpfs/data01/glasslab/home/jtao/software/homer/bin:/gpfs/data01/glasslab/home/jtao/software/weblogo:/home/jtao/code/seq_merge_pipe:/home/vlink/mouse_strains/marge/shifting:/bioinformatics/glassutils/scripts:/bioinformatics/software/meme/bin:/home/jtao/software/lsgkm/bin\n"
     ]
    }
   ],
   "source": [
    "### header ###\n",
    "__author__ = \"Jenhan Tao\"\n",
    "__license__ = \"BSD\"\n",
    "__email__ = \"jenhantao@gmail.com\"\n",
    "\n",
    "### imports ###\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import itertools\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib_venn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from random import shuffle\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter\n",
    "### notebook specific configuration ###\n",
    "%matplotlib inline\n",
    "matplotlib.pylab.rcParams['savefig.dpi'] = 200\n",
    "sys.setrecursionlimit(5000)\n",
    "os.chdir('/gpfs/data01/glasslab/home/jtao/analysis/random_background_analysis/')\n",
    "sns.set_context('notebook')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env PATH=/gpfs/data01/glasslab/home/jtao/perl5/bin:/gpfs/data01/glasslab/home/jtao/software/anaconda3/bin:/home/jtao/software/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/usr/local/bin:/usr/bin:/gpfs/data01/glasslab/home/jtao/software/homer/bin:/gpfs/data01/glasslab/home/jtao/software/weblogo:/home/jtao/code/seq_merge_pipe:/home/vlink/mouse_strains/marge/shifting:/bioinformatics/glassutils/scripts:/bioinformatics/software/meme/bin:/home/jtao/software/lsgkm/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_score_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_sequence_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_strand_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_start_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_end_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/summary_frame.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/annotation_frame.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/peak_sequences/C57BL6J.fa ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_count_frame_C57BL6J.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_summedScore_frame_C57BL6J.pickle ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_score_frame_BALBCJ.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_sequence_frame_BALBCJ.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_strand_frame_BALBCJ.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_start_frame_BALBCJ.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_end_frame_BALBCJ.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/peak_sequences/BALBCJ.fa ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_count_frame_BALBCJ.pickle ./\n",
    "cp /gpfs/data01/glasslab/home/jtao/analysis/cobinding_motif_analysis/motif_summedScore_frame_BALBCJ.pickle ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Score Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C57Bl6 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_frame = pd.read_pickle('summary_frame.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create background peaks from genomic sequences from each chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRandomBackground(target_positions, \n",
    "                        size_ratio = 1.0, \n",
    "                        tolerance = 0.01, \n",
    "                        N_threshold = 0.5 ):\n",
    "    '''\n",
    "    target_sequences: 2D numpy array, list of genomic coordinates for target sequences [[chr,start,end],...]\n",
    "    size_ratio: float, number of background sequences relative to target sequences\n",
    "    tolerance: float, max difference in GC content between True and background labelled samples\n",
    "    *** Uses mm10 genome taken from Homer ***\n",
    "    '''\n",
    "    \n",
    "    ###load mm10 genome into memory\n",
    "    \n",
    "    # index target positions\n",
    "    # {chr:[]}, value is chromosome length boolean array\n",
    "    # largest chromosome has 200 million bps \n",
    "    _chromosomes = ['chr1' , 'chr2' , 'chr3' , 'chr4' , 'chr5' , \n",
    "                    'chr6' , 'chr7' , 'chr8' , 'chr9' , 'chr10', \n",
    "                    'chr11', 'chr12', 'chr13', 'chr14', 'chr15', \n",
    "                    'chr16', 'chr17', 'chr18', 'chr19', 'chrX']\n",
    "    _chrom_size_dict = {}\n",
    "    _chrom_seq_dict = {}\n",
    "    for chrom in _chromosomes:\n",
    "        with open('./mm10_genome/' + chrom + '.fa') as f:\n",
    "            data = f.readlines()\n",
    "        seq = ''.join(x.upper().strip() for x in data[1:])\n",
    "        size = len(seq)\n",
    "        _chrom_size_dict[chrom] = size\n",
    "        _chrom_seq_dict[chrom] = seq\n",
    "    _numChromosomes = len(_chromosomes)\n",
    "    \n",
    "    target_chr_position_dict = {x:np.zeros(200000000) for x in _chromosomes} \n",
    "    ### initialize target_chr_position_dict using target positions\n",
    "    ### retreive target sequences\n",
    "    target_sequences = []\n",
    "    for pos in target_positions:\n",
    "        chrom = pos[0]        \n",
    "        start = pos[1]\n",
    "        end = pos[2]\n",
    "        target_chr_position_dict[chrom][start-1:end] = 1 # use 0 indexing of position, versus 1 indexing used in fasta\n",
    "        seq = _chrom_seq_dict[chrom][start:(end)]\n",
    "        target_sequences.append(seq)\n",
    "    ### calculate GC content and average length of the target sequences\n",
    "    target_gc_count = 0\n",
    "    target_length_count = 0\n",
    "    for s in target_sequences:\n",
    "        target_gc_count += s.count('G')\n",
    "        target_gc_count += s.count('C')\n",
    "        target_length_count += len(s)\n",
    "    target_gc_content = target_gc_count/(target_length_count+0.0000001) # GC content of target sequences\n",
    "    mean_target_length = target_length_count/len(target_sequences) # average length of target sequences\n",
    "    mean_target_length = int(np.floor(mean_target_length))\n",
    "    ### select random genomic loci such that they do no overlap target sequences\n",
    "    numSelected = 0\n",
    "    numToSelect = len(target_positions) * size_ratio # candidate pool of background seqs is size_ratio X larger\n",
    "    candidate_positions = []\n",
    "    numNallowed = int(N_threshold * mean_target_length) # number of allowable Ns\n",
    "    counter = 0\n",
    "    while numSelected < numToSelect:\n",
    "        if counter % 100000 == 0:\n",
    "            print(counter, numSelected)\n",
    "        # select random chromsome\n",
    "        chromIndex = np.random.randint(_numChromosomes)\n",
    "        randChrom = _chromosomes[chromIndex]\n",
    "        randChromSize = _chrom_size_dict[randChrom]\n",
    "        # must find non overlapping segment on this chromosome before moving on\n",
    "        selectedSequence = False\n",
    "        while not selectedSequence:\n",
    "            counter += 1\n",
    "            randStart = np.random.randint(randChromSize)\n",
    "            randEnd = randStart + mean_target_length\n",
    "            overlap_sum = np.sum(target_chr_position_dict[randChrom][randStart:(randEnd + 1)])\n",
    "            \n",
    "            if not overlap_sum > 0:\n",
    "                randSeq = _chrom_seq_dict[randChrom][randStart:(randEnd+1)]\n",
    "                numN = randSeq.count('N')\n",
    "                if numN <= numNallowed:\n",
    "                    rand_gc_count = randSeq.count('G')+ randSeq.count('C')\n",
    "                    rand_gc = rand_gc_count/mean_target_length\n",
    "                    if abs(target_gc_content - rand_gc) <= tolerance:\n",
    "                        selectedSequence = True\n",
    "                        numSelected+=1\n",
    "                        candidate_positions.append([randChrom, randStart, randEnd, randSeq])\n",
    "    # calcuate GC content of background samples\n",
    "    background_gc_count = 0\n",
    "    background_length = 0\n",
    "    for cp in candidate_positions:\n",
    "        s = cp[3]\n",
    "        background_gc_count += s.count('G')\n",
    "        background_gc_count += s.count('C')\n",
    "        background_length += len(s)\n",
    "    background_gc_content = background_gc_count/(background_length+0.0000001)\n",
    "    print('target GC:', target_gc_content, \n",
    "          'background GC:', background_gc_content, \n",
    "          'target length:', mean_target_length,\n",
    "          'numTargetPositions',len(target_positions),\n",
    "          'backgroundPositions', len(candidate_positions))\n",
    "    return candidate_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "target GC: 0.4925952895419081 background GC: 0.4817145044010695 target length: 200 numTargetPositions 23140 backgroundPositions 115700\n",
      "atf3 veh 35.86300587654114\n",
      "0 0\n",
      "100000 31875\n",
      "target GC: 0.4936398738657412 background GC: 0.4817101185622151 target length: 200 numTargetPositions 15539 backgroundPositions 77695\n",
      "cjun veh 30.41100239753723\n",
      "0 0\n",
      "target GC: 0.5973989569748387 background GC: 0.573518327527866 target length: 200 numTargetPositions 767 backgroundPositions 3835\n",
      "fos veh 27.855791807174683\n",
      "0 0\n",
      "target GC: 0.49717002237080854 background GC: 0.48624439324618823 target length: 200 numTargetPositions 447 backgroundPositions 2235\n",
      "junb veh 27.6730477809906\n",
      "0 0\n",
      "target GC: 0.5091324503311129 background GC: 0.4939045475627441 target length: 200 numTargetPositions 19630 backgroundPositions 98150\n",
      "jund veh 32.09514307975769\n",
      "0 0\n",
      "100000 34722\n",
      "200000 69353\n",
      "target GC: 0.4801782310331616 background GC: 0.4734733283097056 target length: 200 numTargetPositions 36722 backgroundPositions 183610\n",
      "atf3 kla 34.09777069091797\n",
      "0 0\n",
      "100000 31862\n",
      "target GC: 0.49073215410153725 background GC: 0.48166472007698363 target length: 200 numTargetPositions 17469 backgroundPositions 87345\n",
      "cjun kla 30.8685200214386\n",
      "0 0\n",
      "100000 33294\n",
      "target GC: 0.48934068416080473 background GC: 0.47745482680637924 target length: 200 numTargetPositions 24351 backgroundPositions 121755\n",
      "fos kla 31.680745124816895\n",
      "0 0\n",
      "target GC: 0.4949569932684931 background GC: 0.48159836568838316 target length: 200 numTargetPositions 13370 backgroundPositions 66850\n",
      "junb kla 30.029738664627075\n",
      "0 0\n",
      "target GC: 0.48878131722130064 background GC: 0.4775397024745925 target length: 200 numTargetPositions 31612 backgroundPositions 158060\n",
      "jund kla 33.10901379585266\n"
     ]
    }
   ],
   "source": [
    "# generate random genomic background for all monomers\n",
    "import time\n",
    "strain = 'c57bl6'\n",
    "ap1_members = ['atf3','cjun', 'fos', 'junb','jund']    \n",
    "for treatment in ['veh', 'kla']:\n",
    "    for monomer in ap1_members:\n",
    "        target_indices = summary_frame[summary_frame[strain + '_' + monomer + '_' + treatment] > 0.0].index.values\n",
    "        target_positions = summary_frame[summary_frame.index.isin(target_indices)][['chr', 'start', 'end']].values\n",
    "        start = time.time()\n",
    "        backgroundPositions = getRandomBackground(target_positions, \n",
    "                                                  N_threshold =1.0, \n",
    "                                                  tolerance=0.05, \n",
    "                                                  size_ratio=5)\n",
    "        end = time.time()\n",
    "        print(monomer, treatment, end - start)\n",
    "        pickle.dump(backgroundPositions,open('./background_pickles/' + monomer + '_' + treatment + '_background.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Background positions to create peak files and Merge Peak Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strain = 'c57bl6'\n",
    "! if [ ! -d ./background_peak_files ]; then mkdir ./background_peak_files; fi\n",
    "ap1_members = ['atf3','cjun', 'fos', 'junb','jund']    \n",
    "\n",
    "for treatment in ['veh', 'kla']:\n",
    "    for monomer in ap1_members:\n",
    "        backgroundPositions = pickle.load(open('./background_pickles/' + monomer + '_' + treatment + '_background.pickle', 'rb'))\n",
    "        outFile = open('./background_peak_files/' + strain + '_' + monomer + '_' + treatment + '-background_peaks.tsv' , 'w')\n",
    "        outFile.write('\\t'.join(['#PeakID','chr','start','end','strand','idrScore', 'count','\\n']))\n",
    "        counter = 0\n",
    "        for pos in backgroundPositions:\n",
    "            chrom = pos[0]\n",
    "            start = str(pos[1])\n",
    "            end = str(pos[2])\n",
    "            strand = '+' # arbitrary - for compatibility with downstream scripts\n",
    "            score = '1' # arbitrary - for compatibility with downstream scripts\n",
    "            randID = 'background_' + str(np.random.randint(10000)) + '_' + str(counter)\n",
    "            counter += 1\n",
    "            outFile.write('\\t'.join([randID, chrom, start, end, strand, score, score, '\\n']))\n",
    "        outFile.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! mergePeaks -d 0 ./background_peak_files/*tsv > ./background_merged_peaks.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no peaks will be merged and so merged peak files will have peak center for both start/end coordinates\n",
    "merged_frame = pd.read_csv('./background_merged_peaks.tsv', sep='\\t', low_memory=False)\n",
    "sizes = [x[2]-x[1] for x in backgroundPositions]\n",
    "mean_peak_size = np.mean(sizes)\n",
    "adj_distance = int(mean_peak_size/2)\n",
    "merged_frame['start'] = merged_frame['start'] - adj_distance\n",
    "merged_frame['end'] = merged_frame['end'] + adj_distance\n",
    "merged_frame.to_csv('./background_merged_fixed_peaks.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! makeSummaryFile.py ./background_merged_fixed_peaks.tsv ./background_group_summary.tsv ./background_peak_files/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data01/glasslab/home/jtao/software/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (5,6,7,8,9,11,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read in peak data data\n",
    "summary_background_frame = pd.read_csv('./background_group_summary.tsv' , sep='\\t')\n",
    "summary_background_frame = summary_background_frame.fillna('0')\n",
    "for col in summary_background_frame.columns[5:]:\n",
    "    floatValues = []\n",
    "    for val in summary_background_frame[col].values.astype(str):\n",
    "        if ',' in val:\n",
    "            maxVal = np.mean([float(x) for x in val.split(',')])\n",
    "            floatValues.append(maxVal)\n",
    "        else:\n",
    "            floatValues.append(float(val))\n",
    "    summary_background_frame[col] = floatValues\n",
    "summary_background_frame.index = summary_background_frame['ID'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a script to scan for motifs using FIMO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d ./peak_sequences ] ;\n",
    "    then mkdir ./peak_sequences\n",
    "else\n",
    "    rm ./peak_sequences/*\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving peaks\n",
      "Loading shift vectors\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "perl /home/vlink/mouse_strains/marge/analysis/extract_seq_from_peakfiles.pl -strains C57BL6J -file ./background_merged_fixed_peaks.tsv -output ./peak_sequences/C57BL6J_marge.fa\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reformat fastq files to use homer peak IDs\n",
    "\n",
    "coordinate_peakID_dict = {} # {chr_start_end:homerID}\n",
    "with open ('./background_merged_fixed_peaks.tsv') as f:\n",
    "    data = f.readlines()\n",
    "for line in data[1:]:\n",
    "    tokens = line.split('\\t')\n",
    "    coordinate = '_'.join(tokens[1:4])\n",
    "    peakID = tokens[0].strip()\n",
    "    coordinate_peakID_dict[coordinate] = tokens[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C57BL6J_marge.fa\n"
     ]
    }
   ],
   "source": [
    "for fastaFile in os.listdir('./peak_sequences/'):\n",
    "    if 'marge' in fastaFile:\n",
    "        strain = fastaFile.split('_')[0]\n",
    "        outFile = open('./peak_sequences/' + fastaFile.replace('_marge',''), 'w')\n",
    "        print(fastaFile)\n",
    "        with open('./peak_sequences/' + fastaFile) as f:\n",
    "            data = f.readlines()\n",
    "        for line in data:\n",
    "            if '>' in line:\n",
    "                coordinate = line[1:].replace('_'+strain,'').strip()\n",
    "                \n",
    "                peakID = coordinate_peakID_dict[coordinate]\n",
    "                outFile.write('>' + peakID + '\\n')\n",
    "               \n",
    "            else:\n",
    "                outFile.write(line)\n",
    "        outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp ./peak_sequences/C57BL6J.fa ./C57BL6J_background.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C57BL6J.fa\n"
     ]
    }
   ],
   "source": [
    "# create a script to scan for motifs using FIMO\n",
    "! if [ ! -d /home/jtao/analysis/random_background_analysis/fimo_results/ ]; then mkdir /home/jtao/analysis/random_background_analysis/fimo_results/; fi\n",
    "! if [ ! -d /home/jtao/analysis/random_background_analysis/fimo_out/ ]; then mkdir /home/jtao/analysis/random_background_analysis/fimo_out/; fi\n",
    "! rm -rf ./fimo_out/*\n",
    "! rm -rf ./fimo_result/*\n",
    "\n",
    "\n",
    "pthresh = 0.01\n",
    "motif_dir = '/home/jtao/analysis/cobinding_motif_analysis/fimo_motifs/'\n",
    "\n",
    "fimo_results_dir = './fimo_results'\n",
    "\n",
    "for fastaFile in os.listdir('./peak_sequences/'):\n",
    "    if not 'marge' in fastaFile:\n",
    "        print(fastaFile)\n",
    "        strain = fastaFile.split('.')[0]\n",
    "        count = 0\n",
    "        scriptFile = open('scanMotifs_background_'+ strain +'.sh','w')\n",
    "        for m in os.listdir(motif_dir):\n",
    "            if 'fimo' in m:\n",
    "                fimo_out_dir = './fimo_out/' + strain + '_' +m.replace('.fimo','')\n",
    "                outPath = fimo_results_dir + '/' +strain + '_' + m.replace('.fimo','') +'.txt'\n",
    "                scriptFile.write(\n",
    "                    '(sleep ' + str(15 * count) + \n",
    "                    's; fimo --text --max-stored-scores 2000000 --output-pthresh ' + \n",
    "                    str(pthresh) +' --oc ' + fimo_out_dir + ' ' +\n",
    "                    motif_dir + '/' + m + ' ./peak_sequences/' + fastaFile +\n",
    "                    '> ' + outPath + ' ) & \\n')\n",
    "                count+=1\n",
    "        scriptFile.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "chmod a+x ./scanMotifs*.sh\n",
    "for i in ./scanMotifs*sh; \n",
    "    do echo 'sleeping...';\n",
    "    echo $i;\n",
    "    $i;\n",
    "#     sleep 5m;\n",
    "done\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Motif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_fimo_file(thread_lock, motif_results_path, \n",
    "                   all_peak_ids,\n",
    "                   motif_score_dict,\n",
    "                   motif_sequence_dict,\n",
    "                   motif_strand_dict,\n",
    "                   motif_start_dict,\n",
    "                   motif_end_dict,\n",
    "                   motif_count_dict,\n",
    "                  ):\n",
    "    \n",
    "    # read in fimo result as data frame\n",
    "    fimo_result_frame=pd.read_csv(motif_results_path, \n",
    "                                  skiprows=1,\n",
    "                                  names=['motif_name', \n",
    "                                         'peak_id', \n",
    "                                         'start', \n",
    "                                         'stop', \n",
    "                                         'strand', \n",
    "                                         'score', \n",
    "                                         'pvalue', \n",
    "                                         'sequence'],\n",
    "                                  sep='\\t')\n",
    "    motif_name = fimo_result_frame['motif_name'].values[0]\n",
    "    print('reading', motif_name)\n",
    "    \n",
    "    id_values_dict = {} # {PeakID:(motifScore, motifSequence, motifStrand, motifStart, motifEnd)}\n",
    "    # drop all motif instances that has less than the maximum score\n",
    "    sorted_fimo_result_frame = fimo_result_frame.sort_values(by='score', ascending=False)\n",
    "    top_fimo_result_frame = sorted_fimo_result_frame.drop_duplicates(subset='peak_id')\n",
    "    \n",
    "    # convert data frame to a dictionary \n",
    "    unique_peak_ids = top_fimo_result_frame['peak_id'].values\n",
    "    scores = top_fimo_result_frame['score'].values\n",
    "    strands = top_fimo_result_frame['strand'].values\n",
    "    sequences = top_fimo_result_frame['sequence'].values\n",
    "    starts = top_fimo_result_frame['start'].values\n",
    "    ends = top_fimo_result_frame['stop'].values\n",
    "    \n",
    "    for i in range(len(unique_peak_ids)):\n",
    "        currentPeakID = unique_peak_ids[i]\n",
    "        currentScore = float(scores[i])\n",
    "        currentSequence = sequences[i]\n",
    "        currentStrand = strands[i]\n",
    "        currentStart = int(starts[i])\n",
    "        currentEnd = int(ends[i])\n",
    "        # bundle values\n",
    "        \n",
    "        if currentScore < 0.0:\n",
    "            currentScore = 0.0\n",
    "        \n",
    "        newValues = (currentScore, \n",
    "                     currentSequence, \n",
    "                     currentStrand, \n",
    "                     currentStart, \n",
    "                     currentEnd, \n",
    "                     )\n",
    "        id_values_dict[currentPeakID] = newValues\n",
    "    \n",
    "    # sort values according to all peak IDs\n",
    "    sorted_values = [id_values_dict[x] if x in id_values_dict else (0,'','?',-1,-1) for x in  all_peak_ids]\n",
    "    sorted_scores = [x[0] for x in sorted_values]\n",
    "    sorted_sequences = [x[1] for x in sorted_values]\n",
    "    sorted_strands = [x[2] for x in sorted_values]\n",
    "    sorted_starts = [x[3] for x in sorted_values]\n",
    "    sorted_ends = [x[4] for x in sorted_values]\n",
    "    \n",
    "    # count the number of motif instances\n",
    "    peak_ids = fimo_result_frame['peak_id'].values\n",
    "    id_count_dict = Counter(peak_ids)\n",
    "    \n",
    "    sorted_counts = [id_count_dict[x] if x in id_count_dict else 0 for x in all_peak_ids]\n",
    "    \n",
    "    while thread_lock.locked_lock():\n",
    "        time.sleep(1)\n",
    "    thread_lock.acquire()\n",
    "        \n",
    "    motif_score_dict[motif_name] = sorted_scores\n",
    "    motif_sequence_dict[motif_name] = sorted_sequences\n",
    "    motif_strand_dict[motif_name] = sorted_strands\n",
    "    motif_start_dict[motif_name] = sorted_starts\n",
    "    motif_end_dict[motif_name] = sorted_ends\n",
    "    motif_count_dict[motif_name] = sorted_counts\n",
    "    # release lock\n",
    "    print('finished reading', motif_name )\n",
    "    if thread_lock.locked_lock():\n",
    "        thread_lock.release()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C57BL6J.fa\n",
      "reading gmeb2\n",
      "reading cux\n",
      "reading arid5a\n",
      "reading hnf1\n",
      "reading homeobox-4\n",
      "reading e2f2\n",
      "reading crem\n",
      "reading homeobox-2\n",
      "reading hoxa11\n",
      "reading gsc\n",
      "reading dbp_hlf_tef\n",
      "reading creb3-l1\n",
      "reading duxa\n",
      "reading alx1_alx4_arx\n",
      "reading arid3a\n",
      "reading arid3b\n",
      "reading mef2a-b-d\n",
      "reading hoxc13\n",
      "reading lhx3\n",
      "reading pax3_pax7\n",
      "reading homeobox-3\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "fimo_result_path = './fimo_results/'\n",
    "for fastaFile in os.listdir('./peak_sequences/'):\n",
    "    if not 'marge' in fastaFile:\n",
    "        print(fastaFile)\n",
    "        strain = fastaFile.split('.')[0]\n",
    "\n",
    "        motif_score_dict = {}\n",
    "        motif_sequence_dict ={}\n",
    "        motif_strand_dict = {}\n",
    "        motif_start_dict = {}\n",
    "        motif_end_dict = {}\n",
    "        motif_count_dict = {}\n",
    "        thread_list = []\n",
    "        all_peak_ids = summary_background_frame.index.values\n",
    "        lock = threading.Lock()\n",
    "        for m in sorted(os.listdir('./fimo_results')):\n",
    "            try:\n",
    "                current_thread = threading.Thread(target = read_fimo_file,\n",
    "                                 args =(lock,\n",
    "                                        fimo_result_path + '/' + m, \n",
    "                                        all_peak_ids,\n",
    "                                        motif_score_dict,\n",
    "                                        motif_sequence_dict,\n",
    "                                        motif_strand_dict,\n",
    "                                        motif_start_dict,\n",
    "                                        motif_end_dict,\n",
    "                                        motif_count_dict,\n",
    "                                        ))\n",
    "                thread_list.append(current_thread)\n",
    "                current_thread.start()\n",
    "            finally:\n",
    "                if lock.locked_lock():\n",
    "                    lock.release()\n",
    "        for current_thread in thread_list:\n",
    "            current_thread.join()\n",
    "\n",
    "        # convert dictionaries to data frames\n",
    "        motif_score_background_frame = pd.DataFrame(motif_score_dict , index = summary_background_frame['ID'].values)\n",
    "        motif_sequence_background_frame = pd.DataFrame(motif_sequence_dict , index = summary_background_frame['ID'].values)\n",
    "        motif_strand_background_frame = pd.DataFrame(motif_strand_dict , index = summary_background_frame['ID'].values)\n",
    "        motif_start_background_frame = pd.DataFrame(motif_start_dict , index = summary_background_frame['ID'].values)\n",
    "        motif_end_background_frame = pd.DataFrame(motif_end_dict , index = summary_background_frame['ID'].values)\n",
    "        motif_count_background_frame = pd.DataFrame(motif_count_dict , index = summary_background_frame['ID'].values)\n",
    "\n",
    "        for frame in [motif_score_background_frame, \n",
    "                      motif_sequence_background_frame, \n",
    "                      motif_strand_background_frame, \n",
    "                      motif_start_background_frame, \n",
    "                      motif_end_background_frame, \n",
    "                      motif_count_background_frame]:\n",
    "            motif_cols = frame.columns.values\n",
    "            frame['ID'] = summary_background_frame['ID'].values\n",
    "            frame['Factors'] = summary_background_frame['Factors'].values\n",
    "            frame['chr'] = summary_background_frame['chr'].values\n",
    "\n",
    "        motif_score_background_frame.to_pickle('motif_score_background_frame_'+  strain + '.pickle3')\n",
    "\n",
    "        motif_sequence_background_frame.to_pickle('motif_sequence_background_frame_'+  strain + '.pickle3')\n",
    "\n",
    "        motif_strand_background_frame.to_pickle('motif_strand_background_frame_'+  strain + '.pickle3')\n",
    "\n",
    "        motif_start_background_frame.to_pickle('motif_start_background_frame_'+  strain + '.pickle3')\n",
    "\n",
    "        motif_end_background_frame.to_pickle('motif_end_background_frame_'+  strain + '.pickle3')\n",
    "\n",
    "        motif_count_background_frame.to_pickle('motif_count_background_frame_'+  strain + '.pickle3')\n",
    "\n",
    "end = time.time()\n",
    "print('total time', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
